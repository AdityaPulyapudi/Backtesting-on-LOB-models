{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M9qS_jUeA0l4",
    "outputId": "a011145c-109f-4156-fea0-64706b97aab6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149, 254750)\n",
      "(149, 203800)\n",
      "(149, 50950)\n",
      "(149, 139587)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "dec_data = np.loadtxt('./Train_Dst_NoAuction_DecPre_CF_7.txt')\n",
    "dec_train = dec_data[:, :int(np.floor(dec_data.shape[1] * 0.8))]\n",
    "dec_val = dec_data[:, int(np.floor(dec_data.shape[1] * 0.8)):]\n",
    "\n",
    "dec_test1 = np.loadtxt('./Test_Dst_NoAuction_DecPre_CF_7.txt')\n",
    "dec_test2 = np.loadtxt('./Test_Dst_NoAuction_DecPre_CF_8.txt')\n",
    "dec_test3 = np.loadtxt('./Test_Dst_NoAuction_DecPre_CF_9.txt')\n",
    "dec_test = np.hstack((dec_test1, dec_test2, dec_test3))\n",
    "\n",
    "print(dec_data.shape)\n",
    "print(dec_train.shape)\n",
    "print(dec_val.shape)\n",
    "print(dec_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QoQe5U4yBOTP"
   },
   "outputs": [],
   "source": [
    "# load packages\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers import Flatten, Dense, Dropout, Activation, Input, LSTM, Reshape, Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import LeakyReLU\n",
    "# from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.utils import to_categorical\n",
    "# from keras.utils import np_utils\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set random seeds\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5a1ZNnSjBQ7q"
   },
   "outputs": [],
   "source": [
    "def prepare_x(data):\n",
    "    df1 = data[:40, :].T\n",
    "    return np.array(df1)\n",
    "\n",
    "def get_label(data):\n",
    "    lob = data[-5:, :].T\n",
    "    return lob\n",
    "\n",
    "def data_classification(X, Y, T):\n",
    "    [N, D] = X.shape\n",
    "    df = np.array(X)\n",
    "    dY = np.array(Y)\n",
    "    dataY = dY[T - 1:N]\n",
    "    dataX = np.zeros((N - T + 1, T, D))\n",
    "    for i in range(T, N + 1):\n",
    "        dataX[i - T] = df[i - T:i, :]\n",
    "    return dataX.reshape(dataX.shape + (1,)), dataY\n",
    "\n",
    "def prepare_x_y(data, k, T):\n",
    "    x = prepare_x(data)\n",
    "    y = get_label(data)\n",
    "    x, y = data_classification(x, y, T=T)\n",
    "    y = y[:,k] - 1\n",
    "    y = to_categorical(y, 3)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "r4FvlMRHGm97"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.W = self.add_weight(name='attention_weight',\n",
    "                                 shape=(input_shape[-1], input_shape[-1]),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        # Calculate attention scores\n",
    "        e = tf.keras.backend.dot(x, self.W)\n",
    "\n",
    "        # Calculate attention weights using softmax activation\n",
    "        a = tf.keras.activations.softmax(e, axis=-2)\n",
    "\n",
    "        # Apply attention weights to the input sequence\n",
    "        output = x * a\n",
    "\n",
    "        # Sum the weighted inputs to get the attention output\n",
    "        output = tf.keras.backend.sum(output, axis=-2)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gE7CLstKBSpr"
   },
   "outputs": [],
   "source": [
    "k = 4 # which prediction horizon\n",
    "T = 100 # the length of a single input i. e lookback_timestep THIS HYPERPARAMETER IS FINE TUNED FOR MODEL CONVERGENCE\n",
    "n_hiddens = 64\n",
    "checkpoint_filepath = './model_tensorflow2/weights'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NubSWmNjBWD4",
    "outputId": "d7b04c4d-3179-4566-b3b3-63d1973a6ca9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149, 203800)\n",
      "(203701, 100, 40, 1) (203701, 3)\n"
     ]
    }
   ],
   "source": [
    "# # Training Dataset Shrinking\n",
    "# dec_train = dec_train[:,:50000]\n",
    "print(dec_train.shape)\n",
    "trainX_CNN, trainY_CNN = prepare_x_y(dec_train, k, T)\n",
    "print(trainX_CNN.shape, trainY_CNN.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bI2kVMq3BYOe",
    "outputId": "14e42588-d304-43a8-b9b2-7debb80f6b52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149, 139587)\n",
      "(139488, 100, 40, 1) (139488, 3)\n"
     ]
    }
   ],
   "source": [
    "# Testing Dataset Shrinking\n",
    "# dec_test = dec_test[:,:20000]\n",
    "print(dec_test.shape)\n",
    "testX_CNN, testY_CNN = prepare_x_y(dec_test, k, T)\n",
    "print(testX_CNN.shape, testY_CNN.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d8WN89lWBZ1p",
    "outputId": "981c9946-93ad-4e5c-dcd0-7a9ab06019c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149, 50950)\n",
      "(50851, 100, 40, 1) (50851, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Validation Dataset Shrinking\n",
    "# dec_val = dec_val[:,:10000]\n",
    "print(dec_val.shape)\n",
    "valX_CNN, valY_CNN = prepare_x_y(dec_val, k, T)\n",
    "print(valX_CNN.shape, valY_CNN.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_vUIbc0zBbvk",
    "outputId": "a61015d4-8b4a-42d3-eaf4-2200bc6e7f2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(203701, 100, 40, 1) (203701, 3)\n",
      "(50851, 100, 40, 1) (50851, 3)\n",
      "(139488, 100, 40, 1) (139488, 3)\n"
     ]
    }
   ],
   "source": [
    "print(trainX_CNN.shape, trainY_CNN.shape)\n",
    "print(valX_CNN.shape, valY_CNN.shape)\n",
    "print(testX_CNN.shape, testY_CNN.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mV1o2TQOE18o",
    "outputId": "5a7c6cf4-ff06-44bf-cbfb-30d0b56d62b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 100, 40, 1)]         0         []                            \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 100, 20, 32)          96        ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)     (None, 100, 20, 32)          0         ['conv2d[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 100, 20, 32)          4128      ['leaky_re_lu[0][0]']         \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 100, 20, 32)          0         ['conv2d_1[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 100, 20, 32)          4128      ['leaky_re_lu_1[0][0]']       \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 100, 20, 32)          0         ['conv2d_2[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 100, 10, 32)          2080      ['leaky_re_lu_2[0][0]']       \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 100, 10, 32)          0         ['conv2d_3[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, 100, 10, 32)          4128      ['leaky_re_lu_3[0][0]']       \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 100, 10, 32)          0         ['conv2d_4[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)           (None, 100, 10, 32)          4128      ['leaky_re_lu_4[0][0]']       \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 100, 10, 32)          0         ['conv2d_5[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)           (None, 100, 1, 32)           10272     ['leaky_re_lu_5[0][0]']       \n",
      "                                                                                                  \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 100, 1, 32)           0         ['conv2d_6[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)           (None, 100, 1, 32)           4128      ['leaky_re_lu_6[0][0]']       \n",
      "                                                                                                  \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, 100, 1, 32)           0         ['conv2d_7[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)           (None, 100, 1, 32)           4128      ['leaky_re_lu_7[0][0]']       \n",
      "                                                                                                  \n",
      " leaky_re_lu_8 (LeakyReLU)   (None, 100, 1, 32)           0         ['conv2d_8[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)           (None, 100, 1, 64)           2112      ['leaky_re_lu_8[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)          (None, 100, 1, 64)           2112      ['leaky_re_lu_8[0][0]']       \n",
      "                                                                                                  \n",
      " leaky_re_lu_9 (LeakyReLU)   (None, 100, 1, 64)           0         ['conv2d_9[0][0]']            \n",
      "                                                                                                  \n",
      " leaky_re_lu_11 (LeakyReLU)  (None, 100, 1, 64)           0         ['conv2d_11[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2  (None, 100, 1, 32)           0         ['leaky_re_lu_8[0][0]']       \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)          (None, 100, 1, 64)           12352     ['leaky_re_lu_9[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)          (None, 100, 1, 64)           20544     ['leaky_re_lu_11[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)          (None, 100, 1, 64)           2112      ['max_pooling2d[0][0]']       \n",
      "                                                                                                  \n",
      " leaky_re_lu_10 (LeakyReLU)  (None, 100, 1, 64)           0         ['conv2d_10[0][0]']           \n",
      "                                                                                                  \n",
      " leaky_re_lu_12 (LeakyReLU)  (None, 100, 1, 64)           0         ['conv2d_12[0][0]']           \n",
      "                                                                                                  \n",
      " leaky_re_lu_13 (LeakyReLU)  (None, 100, 1, 64)           0         ['conv2d_13[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 100, 1, 192)          0         ['leaky_re_lu_10[0][0]',      \n",
      "                                                                     'leaky_re_lu_12[0][0]',      \n",
      "                                                                     'leaky_re_lu_13[0][0]']      \n",
      "                                                                                                  \n",
      " reshape (Reshape)           (None, 100, 192)             0         ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 100, 192)             0         ['reshape[0][0]']             \n",
      "                                                                                                  \n",
      " bidirectional (Bidirection  (None, 100, 128)             131584    ['dropout[0][0]']             \n",
      " al)                                                                                              \n",
      "                                                                                                  \n",
      " attention_layer (Attention  (None, 128)                  16384     ['bidirectional[0][0]']       \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " repeat_vector (RepeatVecto  (None, 100, 128)             0         ['attention_layer[0][0]']     \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)               (None, 100, 64)              49408     ['repeat_vector[0][0]']       \n",
      "                                                                                                  \n",
      " time_distributed (TimeDist  (None, 100, 3)               195       ['lstm_1[0][0]']              \n",
      " ributed)                                                                                         \n",
      "                                                                                                  \n",
      " flatten (Flatten)           (None, 300)                  0         ['time_distributed[0][0]']    \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 3)                    903       ['flatten[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 274922 (1.05 MB)\n",
      "Trainable params: 274922 (1.05 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Bidirectional, LSTM, RepeatVector, TimeDistributed\n",
    "\n",
    "def create_deeplob_encoder_decoder(T, NF, number_of_lstm):\n",
    "    input_lmd = Input(shape=(T, NF, 1))\n",
    "\n",
    "    # build the convolutional block\n",
    "    conv_first1 = Conv2D(32, (1, 2), strides=(1, 2))(input_lmd)\n",
    "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    conv_first1 = Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
    "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    conv_first1 = Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
    "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
    "\n",
    "    conv_first1 = Conv2D(32, (1, 2), strides=(1, 2))(conv_first1)\n",
    "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    conv_first1 = Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
    "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    conv_first1 = Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
    "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
    "\n",
    "    conv_first1 = Conv2D(32, (1, 10))(conv_first1)\n",
    "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    conv_first1 = Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
    "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    conv_first1 = Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
    "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
    "\n",
    "    # build the inception module\n",
    "    convsecond_1 = Conv2D(64, (1, 1), padding='same')(conv_first1)\n",
    "    convsecond_1 = keras.layers.LeakyReLU(alpha=0.01)(convsecond_1)\n",
    "    convsecond_1 = Conv2D(64, (3, 1), padding='same')(convsecond_1)\n",
    "    convsecond_1 = keras.layers.LeakyReLU(alpha=0.01)(convsecond_1)\n",
    "\n",
    "    convsecond_2 = Conv2D(64, (1, 1), padding='same')(conv_first1)\n",
    "    convsecond_2 = keras.layers.LeakyReLU(alpha=0.01)(convsecond_2)\n",
    "    convsecond_2 = Conv2D(64, (5, 1), padding='same')(convsecond_2)\n",
    "    convsecond_2 = keras.layers.LeakyReLU(alpha=0.01)(convsecond_2)\n",
    "\n",
    "    convsecond_3 = MaxPooling2D((3, 1), strides=(1, 1), padding='same')(conv_first1)\n",
    "    convsecond_3 = Conv2D(64, (1, 1), padding='same')(convsecond_3)\n",
    "    convsecond_3 = keras.layers.LeakyReLU(alpha=0.01)(convsecond_3)\n",
    "\n",
    "    convsecond_output = keras.layers.concatenate([convsecond_1, convsecond_2, convsecond_3], axis=3)\n",
    "    conv_reshape = Reshape((int(convsecond_output.shape[1]), int(convsecond_output.shape[3])))(convsecond_output)\n",
    "    conv_reshape = keras.layers.Dropout(0.2, noise_shape=(None, 1, int(conv_reshape.shape[2])))(conv_reshape, training=True)\n",
    "\n",
    "    # Build the encoder (Bidirectional LSTM)\n",
    "    encoder_output = Bidirectional(LSTM(number_of_lstm, return_sequences=True))(conv_reshape)\n",
    "\n",
    "    # Attention layer\n",
    "    attention_output = AttentionLayer()(encoder_output)\n",
    "\n",
    "    # Repeat vector to match the time steps for the decoder\n",
    "    repeated_vector = RepeatVector(T)(attention_output)\n",
    "\n",
    "    # Decoder (LSTM)\n",
    "    decoder_output = LSTM(number_of_lstm, return_sequences=True)(repeated_vector)\n",
    "\n",
    "    # TimeDistributed layer to apply Dense to every time step of the sequence\n",
    "    output = TimeDistributed(Dense(3, activation='softmax'))(decoder_output)\n",
    "\n",
    "    # Flatten the output before the final Dense layer\n",
    "    flatten_output = Flatten()(output)  # output is the output of the TimeDistributed(Dense(3, activation='softmax'))\n",
    "\n",
    "    # Final Dense layer for output\n",
    "    final_output = Dense(3, activation='softmax')(flatten_output)\n",
    "\n",
    "    model = Model(inputs=input_lmd, outputs=final_output)\n",
    "    adam = keras.optimizers.Adam(learning_rate=0.0001)  # Update the optimizer\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "deeplob_encoder_decoder = create_deeplob_encoder_decoder(trainX_CNN.shape[1], trainX_CNN.shape[2], n_hiddens)\n",
    "deeplob_encoder_decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9fbF-YRWBgG7",
    "outputId": "c2ee14db-8207-4603-f9fc-581b6fbc9d7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1592/1592 - 1516s - loss: 1.0292 - accuracy: 0.4164 - val_loss: 1.0882 - val_accuracy: 0.3723 - 1516s/epoch - 952ms/step\n",
      "Epoch 2/200\n",
      "1592/1592 - 1492s - loss: 1.0183 - accuracy: 0.4212 - val_loss: 1.0970 - val_accuracy: 0.3744 - 1492s/epoch - 937ms/step\n",
      "Epoch 3/200\n",
      "1592/1592 - 1515s - loss: 1.0182 - accuracy: 0.4218 - val_loss: 1.0905 - val_accuracy: 0.3729 - 1515s/epoch - 952ms/step\n",
      "Epoch 4/200\n",
      "1592/1592 - 1520s - loss: 1.0147 - accuracy: 0.4231 - val_loss: 1.0889 - val_accuracy: 0.3681 - 1520s/epoch - 955ms/step\n",
      "Epoch 5/200\n",
      "1592/1592 - 1540s - loss: 1.0085 - accuracy: 0.4303 - val_loss: 1.0925 - val_accuracy: 0.3729 - 1540s/epoch - 968ms/step\n",
      "Epoch 6/200\n",
      "1592/1592 - 1540s - loss: 0.9258 - accuracy: 0.4650 - val_loss: 1.0814 - val_accuracy: 0.3798 - 1540s/epoch - 967ms/step\n",
      "Epoch 7/200\n",
      "1592/1592 - 1533s - loss: 0.8978 - accuracy: 0.4780 - val_loss: 1.0541 - val_accuracy: 0.4025 - 1533s/epoch - 963ms/step\n",
      "Epoch 8/200\n",
      "1592/1592 - 1530s - loss: 0.8822 - accuracy: 0.4877 - val_loss: 1.0259 - val_accuracy: 0.4217 - 1530s/epoch - 961ms/step\n",
      "Epoch 9/200\n",
      "1592/1592 - 1516s - loss: 0.8650 - accuracy: 0.4992 - val_loss: 1.0114 - val_accuracy: 0.4567 - 1516s/epoch - 952ms/step\n",
      "Epoch 10/200\n",
      "1592/1592 - 1526s - loss: 0.8489 - accuracy: 0.5059 - val_loss: 0.9596 - val_accuracy: 0.4718 - 1526s/epoch - 959ms/step\n",
      "Epoch 11/200\n",
      "1592/1592 - 1528s - loss: 0.8423 - accuracy: 0.5107 - val_loss: 0.9278 - val_accuracy: 0.4885 - 1528s/epoch - 960ms/step\n",
      "Epoch 12/200\n",
      "1592/1592 - 1530s - loss: 0.8347 - accuracy: 0.5185 - val_loss: 0.9054 - val_accuracy: 0.5035 - 1530s/epoch - 961ms/step\n",
      "Epoch 13/200\n",
      "1592/1592 - 1553s - loss: 0.8288 - accuracy: 0.5320 - val_loss: 0.9083 - val_accuracy: 0.5127 - 1553s/epoch - 976ms/step\n",
      "Epoch 14/200\n",
      "1592/1592 - 1505s - loss: 0.7992 - accuracy: 0.5849 - val_loss: 0.8998 - val_accuracy: 0.5184 - 1505s/epoch - 945ms/step\n",
      "Epoch 15/200\n",
      "1592/1592 - 1517s - loss: 0.7420 - accuracy: 0.6446 - val_loss: 0.9082 - val_accuracy: 0.5299 - 1517s/epoch - 953ms/step\n",
      "Epoch 16/200\n",
      "1592/1592 - 1540s - loss: 0.7021 - accuracy: 0.6783 - val_loss: 0.8757 - val_accuracy: 0.5684 - 1540s/epoch - 967ms/step\n",
      "Epoch 17/200\n",
      "1592/1592 - 1551s - loss: 0.6628 - accuracy: 0.7055 - val_loss: 0.8791 - val_accuracy: 0.5779 - 1551s/epoch - 974ms/step\n",
      "Epoch 18/200\n",
      "1592/1592 - 1558s - loss: 0.6320 - accuracy: 0.7250 - val_loss: 0.8675 - val_accuracy: 0.5844 - 1558s/epoch - 979ms/step\n",
      "Epoch 19/200\n",
      "1592/1592 - 1551s - loss: 0.6084 - accuracy: 0.7397 - val_loss: 0.8596 - val_accuracy: 0.5933 - 1551s/epoch - 974ms/step\n",
      "Epoch 20/200\n",
      "1592/1592 - 1546s - loss: 0.5895 - accuracy: 0.7502 - val_loss: 0.8669 - val_accuracy: 0.5923 - 1546s/epoch - 971ms/step\n",
      "Epoch 21/200\n",
      "1592/1592 - 1513s - loss: 0.5741 - accuracy: 0.7593 - val_loss: 0.8623 - val_accuracy: 0.5891 - 1513s/epoch - 950ms/step\n",
      "Epoch 22/200\n",
      "1592/1592 - 1516s - loss: 0.5592 - accuracy: 0.7674 - val_loss: 0.8648 - val_accuracy: 0.6022 - 1516s/epoch - 953ms/step\n",
      "Epoch 23/200\n",
      "1592/1592 - 1486s - loss: 0.5471 - accuracy: 0.7754 - val_loss: 0.8534 - val_accuracy: 0.6046 - 1486s/epoch - 933ms/step\n",
      "Epoch 24/200\n",
      "1592/1592 - 1509s - loss: 0.5354 - accuracy: 0.7816 - val_loss: 0.8655 - val_accuracy: 0.5945 - 1509s/epoch - 948ms/step\n",
      "Epoch 25/200\n",
      "1592/1592 - 1493s - loss: 0.5246 - accuracy: 0.7877 - val_loss: 0.8606 - val_accuracy: 0.6072 - 1493s/epoch - 938ms/step\n",
      "Epoch 26/200\n",
      "1592/1592 - 1492s - loss: 0.5152 - accuracy: 0.7929 - val_loss: 0.8670 - val_accuracy: 0.6034 - 1492s/epoch - 937ms/step\n",
      "Epoch 27/200\n",
      "1592/1592 - 1503s - loss: 0.5070 - accuracy: 0.7971 - val_loss: 0.8546 - val_accuracy: 0.6199 - 1503s/epoch - 944ms/step\n",
      "Epoch 28/200\n",
      "1592/1592 - 1490s - loss: 0.4979 - accuracy: 0.8021 - val_loss: 0.8873 - val_accuracy: 0.6112 - 1490s/epoch - 936ms/step\n",
      "Epoch 29/200\n",
      "1592/1592 - 1506s - loss: 0.4917 - accuracy: 0.8055 - val_loss: 0.8642 - val_accuracy: 0.6229 - 1506s/epoch - 946ms/step\n",
      "Epoch 30/200\n",
      "1592/1592 - 1476s - loss: 0.4827 - accuracy: 0.8106 - val_loss: 0.8705 - val_accuracy: 0.6204 - 1476s/epoch - 927ms/step\n",
      "Epoch 31/200\n",
      "1592/1592 - 1501s - loss: 0.4747 - accuracy: 0.8142 - val_loss: 0.8867 - val_accuracy: 0.6244 - 1501s/epoch - 943ms/step\n",
      "Epoch 32/200\n",
      "1592/1592 - 1520s - loss: 0.4692 - accuracy: 0.8169 - val_loss: 0.8683 - val_accuracy: 0.6283 - 1520s/epoch - 955ms/step\n",
      "Epoch 33/200\n",
      "1592/1592 - 1523s - loss: 0.4617 - accuracy: 0.8213 - val_loss: 0.8772 - val_accuracy: 0.6220 - 1523s/epoch - 957ms/step\n",
      "Epoch 34/200\n",
      "1592/1592 - 1517s - loss: 0.4563 - accuracy: 0.8236 - val_loss: 0.8525 - val_accuracy: 0.6323 - 1517s/epoch - 953ms/step\n",
      "Epoch 35/200\n",
      "1592/1592 - 1509s - loss: 0.4492 - accuracy: 0.8267 - val_loss: 0.8854 - val_accuracy: 0.6305 - 1509s/epoch - 948ms/step\n",
      "Epoch 36/200\n",
      "1592/1592 - 1516s - loss: 0.4445 - accuracy: 0.8290 - val_loss: 0.8911 - val_accuracy: 0.6246 - 1516s/epoch - 952ms/step\n",
      "Epoch 37/200\n",
      "1592/1592 - 1517s - loss: 0.4386 - accuracy: 0.8320 - val_loss: 0.8854 - val_accuracy: 0.6321 - 1517s/epoch - 953ms/step\n",
      "Epoch 38/200\n",
      "1592/1592 - 1511s - loss: 0.4331 - accuracy: 0.8342 - val_loss: 0.9008 - val_accuracy: 0.6286 - 1511s/epoch - 949ms/step\n",
      "Epoch 39/200\n",
      "1592/1592 - 1501s - loss: 0.4270 - accuracy: 0.8369 - val_loss: 0.8947 - val_accuracy: 0.6325 - 1501s/epoch - 943ms/step\n",
      "Epoch 40/200\n",
      "1592/1592 - 1490s - loss: 0.4226 - accuracy: 0.8391 - val_loss: 0.8873 - val_accuracy: 0.6270 - 1490s/epoch - 936ms/step\n",
      "Epoch 41/200\n",
      "1592/1592 - 1513s - loss: 0.4180 - accuracy: 0.8413 - val_loss: 0.8901 - val_accuracy: 0.6358 - 1513s/epoch - 950ms/step\n",
      "Epoch 42/200\n",
      "1592/1592 - 1509s - loss: 0.4137 - accuracy: 0.8437 - val_loss: 0.9431 - val_accuracy: 0.6293 - 1509s/epoch - 948ms/step\n",
      "Epoch 43/200\n",
      "1592/1592 - 1514s - loss: 0.4090 - accuracy: 0.8452 - val_loss: 0.9009 - val_accuracy: 0.6389 - 1514s/epoch - 951ms/step\n",
      "Epoch 44/200\n",
      "1592/1592 - 1515s - loss: 0.4054 - accuracy: 0.8468 - val_loss: 0.9310 - val_accuracy: 0.6346 - 1515s/epoch - 952ms/step\n",
      "Epoch 45/200\n",
      "1592/1592 - 1516s - loss: 0.4008 - accuracy: 0.8492 - val_loss: 0.9641 - val_accuracy: 0.6219 - 1516s/epoch - 952ms/step\n",
      "Epoch 46/200\n",
      "1592/1592 - 1523s - loss: 0.3959 - accuracy: 0.8521 - val_loss: 0.9542 - val_accuracy: 0.6262 - 1523s/epoch - 956ms/step\n",
      "Epoch 47/200\n",
      "1592/1592 - 1549s - loss: 0.3924 - accuracy: 0.8523 - val_loss: 0.9321 - val_accuracy: 0.6320 - 1549s/epoch - 973ms/step\n",
      "Epoch 48/200\n",
      "1592/1592 - 1525s - loss: 0.3893 - accuracy: 0.8541 - val_loss: 0.9794 - val_accuracy: 0.6314 - 1525s/epoch - 958ms/step\n",
      "Epoch 49/200\n",
      "1592/1592 - 1526s - loss: 0.3858 - accuracy: 0.8561 - val_loss: 0.9377 - val_accuracy: 0.6270 - 1526s/epoch - 959ms/step\n",
      "Epoch 50/200\n",
      "1592/1592 - 1517s - loss: 0.3816 - accuracy: 0.8572 - val_loss: 0.9601 - val_accuracy: 0.6312 - 1517s/epoch - 953ms/step\n",
      "Epoch 51/200\n",
      "1592/1592 - 1505s - loss: 0.3781 - accuracy: 0.8590 - val_loss: 0.9569 - val_accuracy: 0.6311 - 1505s/epoch - 945ms/step\n",
      "Epoch 52/200\n",
      "1592/1592 - 1526s - loss: 0.3750 - accuracy: 0.8606 - val_loss: 0.9654 - val_accuracy: 0.6252 - 1526s/epoch - 958ms/step\n",
      "Epoch 53/200\n",
      "1592/1592 - 1512s - loss: 0.3719 - accuracy: 0.8618 - val_loss: 0.9730 - val_accuracy: 0.6269 - 1512s/epoch - 950ms/step\n",
      "Epoch 54/200\n",
      "1592/1592 - 1528s - loss: 0.3680 - accuracy: 0.8638 - val_loss: 1.0288 - val_accuracy: 0.6269 - 1528s/epoch - 960ms/step\n",
      "Epoch 55/200\n",
      "1592/1592 - 1555s - loss: 0.3653 - accuracy: 0.8643 - val_loss: 0.9718 - val_accuracy: 0.6252 - 1555s/epoch - 977ms/step\n",
      "Epoch 56/200\n",
      "1592/1592 - 1500s - loss: 0.3628 - accuracy: 0.8656 - val_loss: 0.9822 - val_accuracy: 0.6317 - 1500s/epoch - 943ms/step\n",
      "Epoch 57/200\n",
      "1592/1592 - 1530s - loss: 0.3592 - accuracy: 0.8676 - val_loss: 1.0026 - val_accuracy: 0.6224 - 1530s/epoch - 961ms/step\n",
      "Epoch 58/200\n",
      "1592/1592 - 1507s - loss: 0.3567 - accuracy: 0.8683 - val_loss: 1.0084 - val_accuracy: 0.6237 - 1507s/epoch - 947ms/step\n",
      "Epoch 59/200\n",
      "1592/1592 - 1521s - loss: 0.3545 - accuracy: 0.8687 - val_loss: 1.0419 - val_accuracy: 0.6175 - 1521s/epoch - 956ms/step\n",
      "Epoch 60/200\n",
      "1592/1592 - 1488s - loss: 0.3504 - accuracy: 0.8704 - val_loss: 1.0136 - val_accuracy: 0.6227 - 1488s/epoch - 935ms/step\n",
      "Epoch 61/200\n",
      "1592/1592 - 1511s - loss: 0.3480 - accuracy: 0.8720 - val_loss: 1.0697 - val_accuracy: 0.6209 - 1511s/epoch - 949ms/step\n",
      "Epoch 62/200\n",
      "1592/1592 - 1512s - loss: 0.3455 - accuracy: 0.8737 - val_loss: 1.0385 - val_accuracy: 0.6252 - 1512s/epoch - 950ms/step\n",
      "Epoch 63/200\n",
      "1592/1592 - 1496s - loss: 0.3432 - accuracy: 0.8738 - val_loss: 1.0486 - val_accuracy: 0.6272 - 1496s/epoch - 940ms/step\n",
      "Epoch 64/200\n",
      "1592/1592 - 1531s - loss: 0.3404 - accuracy: 0.8753 - val_loss: 1.0281 - val_accuracy: 0.6274 - 1531s/epoch - 961ms/step\n",
      "Epoch 65/200\n",
      "1592/1592 - 1516s - loss: 0.3385 - accuracy: 0.8754 - val_loss: 1.0501 - val_accuracy: 0.6249 - 1516s/epoch - 952ms/step\n",
      "Epoch 66/200\n",
      "1592/1592 - 1513s - loss: 0.3348 - accuracy: 0.8770 - val_loss: 1.0738 - val_accuracy: 0.6266 - 1513s/epoch - 951ms/step\n",
      "Epoch 67/200\n",
      "1592/1592 - 1518s - loss: 0.3339 - accuracy: 0.8773 - val_loss: 1.0599 - val_accuracy: 0.6256 - 1518s/epoch - 953ms/step\n",
      "Epoch 68/200\n",
      "1592/1592 - 1535s - loss: 0.3301 - accuracy: 0.8790 - val_loss: 1.0601 - val_accuracy: 0.6202 - 1535s/epoch - 964ms/step\n",
      "Epoch 69/200\n",
      "1592/1592 - 1530s - loss: 0.3286 - accuracy: 0.8795 - val_loss: 1.0812 - val_accuracy: 0.6182 - 1530s/epoch - 961ms/step\n",
      "Epoch 70/200\n",
      "1592/1592 - 1516s - loss: 0.3264 - accuracy: 0.8803 - val_loss: 1.0566 - val_accuracy: 0.6186 - 1516s/epoch - 953ms/step\n",
      "Epoch 71/200\n",
      "1592/1592 - 1516s - loss: 0.3224 - accuracy: 0.8820 - val_loss: 1.1115 - val_accuracy: 0.6165 - 1516s/epoch - 952ms/step\n",
      "Epoch 72/200\n",
      "1592/1592 - 1515s - loss: 0.3224 - accuracy: 0.8820 - val_loss: 1.0884 - val_accuracy: 0.6169 - 1515s/epoch - 952ms/step\n",
      "Epoch 73/200\n",
      "1592/1592 - 1511s - loss: 0.3193 - accuracy: 0.8830 - val_loss: 1.0865 - val_accuracy: 0.6207 - 1511s/epoch - 949ms/step\n",
      "Epoch 74/200\n",
      "1592/1592 - 1530s - loss: 0.3170 - accuracy: 0.8843 - val_loss: 1.1145 - val_accuracy: 0.6217 - 1530s/epoch - 961ms/step\n",
      "Epoch 75/200\n",
      "1592/1592 - 1485s - loss: 0.3148 - accuracy: 0.8853 - val_loss: 1.1212 - val_accuracy: 0.6143 - 1485s/epoch - 933ms/step\n",
      "Epoch 76/200\n",
      "1592/1592 - 1499s - loss: 0.3126 - accuracy: 0.8858 - val_loss: 1.1430 - val_accuracy: 0.6200 - 1499s/epoch - 941ms/step\n",
      "Epoch 77/200\n",
      "1592/1592 - 1513s - loss: 0.3112 - accuracy: 0.8854 - val_loss: 1.1470 - val_accuracy: 0.6179 - 1513s/epoch - 951ms/step\n",
      "Epoch 78/200\n",
      "1592/1592 - 1522s - loss: 0.3099 - accuracy: 0.8867 - val_loss: 1.1216 - val_accuracy: 0.6186 - 1522s/epoch - 956ms/step\n",
      "Epoch 79/200\n",
      "1592/1592 - 1528s - loss: 0.3066 - accuracy: 0.8878 - val_loss: 1.1002 - val_accuracy: 0.6127 - 1528s/epoch - 960ms/step\n",
      "Epoch 80/200\n",
      "1592/1592 - 1522s - loss: 0.3045 - accuracy: 0.8890 - val_loss: 1.1694 - val_accuracy: 0.6126 - 1522s/epoch - 956ms/step\n",
      "Epoch 81/200\n",
      "1592/1592 - 1520s - loss: 0.3036 - accuracy: 0.8895 - val_loss: 1.1605 - val_accuracy: 0.6199 - 1520s/epoch - 955ms/step\n",
      "Epoch 82/200\n",
      "1592/1592 - 1514s - loss: 0.3006 - accuracy: 0.8906 - val_loss: 1.1547 - val_accuracy: 0.6186 - 1514s/epoch - 951ms/step\n",
      "Epoch 83/200\n",
      "1592/1592 - 1531s - loss: 0.2993 - accuracy: 0.8911 - val_loss: 1.1784 - val_accuracy: 0.6168 - 1531s/epoch - 961ms/step\n",
      "Epoch 84/200\n",
      "1592/1592 - 1560s - loss: 0.2981 - accuracy: 0.8913 - val_loss: 1.1558 - val_accuracy: 0.6197 - 1560s/epoch - 980ms/step\n",
      "Epoch 85/200\n",
      "1592/1592 - 1563s - loss: 0.2950 - accuracy: 0.8925 - val_loss: 1.1440 - val_accuracy: 0.6151 - 1563s/epoch - 982ms/step\n",
      "Epoch 86/200\n",
      "1592/1592 - 1563s - loss: 0.2936 - accuracy: 0.8929 - val_loss: 1.1855 - val_accuracy: 0.6073 - 1563s/epoch - 982ms/step\n",
      "Epoch 87/200\n",
      "1592/1592 - 1559s - loss: 0.2904 - accuracy: 0.8942 - val_loss: 1.2025 - val_accuracy: 0.6090 - 1559s/epoch - 979ms/step\n",
      "Epoch 88/200\n",
      "1592/1592 - 1551s - loss: 0.2906 - accuracy: 0.8939 - val_loss: 1.1947 - val_accuracy: 0.6150 - 1551s/epoch - 974ms/step\n",
      "Epoch 89/200\n",
      "1592/1592 - 1532s - loss: 0.2886 - accuracy: 0.8949 - val_loss: 1.1656 - val_accuracy: 0.6143 - 1532s/epoch - 962ms/step\n",
      "Epoch 90/200\n",
      "1592/1592 - 1529s - loss: 0.2860 - accuracy: 0.8958 - val_loss: 1.1874 - val_accuracy: 0.6146 - 1529s/epoch - 961ms/step\n",
      "Epoch 91/200\n",
      "1592/1592 - 1541s - loss: 0.2843 - accuracy: 0.8967 - val_loss: 1.1671 - val_accuracy: 0.6225 - 1541s/epoch - 968ms/step\n",
      "Epoch 92/200\n",
      "1592/1592 - 1538s - loss: 0.2830 - accuracy: 0.8972 - val_loss: 1.2149 - val_accuracy: 0.6126 - 1538s/epoch - 966ms/step\n",
      "Epoch 93/200\n",
      "1592/1592 - 1546s - loss: 0.2805 - accuracy: 0.8983 - val_loss: 1.2429 - val_accuracy: 0.6149 - 1546s/epoch - 971ms/step\n",
      "Epoch 94/200\n",
      "1592/1592 - 1539s - loss: 0.2791 - accuracy: 0.8987 - val_loss: 1.2167 - val_accuracy: 0.6097 - 1539s/epoch - 967ms/step\n",
      "Epoch 95/200\n",
      "1592/1592 - 1531s - loss: 0.2779 - accuracy: 0.8990 - val_loss: 1.2404 - val_accuracy: 0.6094 - 1531s/epoch - 962ms/step\n",
      "Epoch 96/200\n",
      "1592/1592 - 1497s - loss: 0.2762 - accuracy: 0.8997 - val_loss: 1.2193 - val_accuracy: 0.6132 - 1497s/epoch - 940ms/step\n",
      "Epoch 97/200\n",
      "1592/1592 - 1511s - loss: 0.2744 - accuracy: 0.9005 - val_loss: 1.2315 - val_accuracy: 0.6125 - 1511s/epoch - 949ms/step\n",
      "Epoch 98/200\n",
      "1592/1592 - 1510s - loss: 0.2737 - accuracy: 0.9007 - val_loss: 1.1959 - val_accuracy: 0.6151 - 1510s/epoch - 948ms/step\n",
      "Epoch 99/200\n",
      "1592/1592 - 1492s - loss: 0.2707 - accuracy: 0.9015 - val_loss: 1.2774 - val_accuracy: 0.6107 - 1492s/epoch - 937ms/step\n",
      "Epoch 100/200\n",
      "1592/1592 - 1498s - loss: 0.2711 - accuracy: 0.9015 - val_loss: 1.2553 - val_accuracy: 0.6127 - 1498s/epoch - 941ms/step\n",
      "Epoch 101/200\n",
      "1592/1592 - 1481s - loss: 0.2685 - accuracy: 0.9024 - val_loss: 1.2805 - val_accuracy: 0.6146 - 1481s/epoch - 930ms/step\n",
      "Epoch 102/200\n",
      "1592/1592 - 1512s - loss: 0.2675 - accuracy: 0.9027 - val_loss: 1.2897 - val_accuracy: 0.6058 - 1512s/epoch - 950ms/step\n",
      "Epoch 103/200\n",
      "1592/1592 - 1519s - loss: 0.2658 - accuracy: 0.9034 - val_loss: 1.2106 - val_accuracy: 0.6061 - 1519s/epoch - 954ms/step\n",
      "Epoch 104/200\n",
      "1592/1592 - 1518s - loss: 0.2645 - accuracy: 0.9038 - val_loss: 1.2484 - val_accuracy: 0.6170 - 1518s/epoch - 954ms/step\n",
      "Epoch 105/200\n",
      "1592/1592 - 1499s - loss: 0.2643 - accuracy: 0.9039 - val_loss: 1.2561 - val_accuracy: 0.6151 - 1499s/epoch - 941ms/step\n",
      "Epoch 106/200\n",
      "1592/1592 - 1503s - loss: 0.2613 - accuracy: 0.9047 - val_loss: 1.2713 - val_accuracy: 0.6160 - 1503s/epoch - 944ms/step\n",
      "Epoch 107/200\n",
      "1592/1592 - 1492s - loss: 0.2604 - accuracy: 0.9052 - val_loss: 1.2625 - val_accuracy: 0.6174 - 1492s/epoch - 937ms/step\n",
      "Epoch 108/200\n",
      "1592/1592 - 1497s - loss: 0.2587 - accuracy: 0.9062 - val_loss: 1.2693 - val_accuracy: 0.6111 - 1497s/epoch - 940ms/step\n",
      "Epoch 109/200\n",
      "1592/1592 - 1485s - loss: 0.2571 - accuracy: 0.9069 - val_loss: 1.2698 - val_accuracy: 0.6121 - 1485s/epoch - 933ms/step\n",
      "Epoch 110/200\n",
      "1592/1592 - 1497s - loss: 0.2562 - accuracy: 0.9066 - val_loss: 1.2617 - val_accuracy: 0.6129 - 1497s/epoch - 940ms/step\n",
      "Epoch 111/200\n",
      "1592/1592 - 1487s - loss: 0.2542 - accuracy: 0.9081 - val_loss: 1.2451 - val_accuracy: 0.6114 - 1487s/epoch - 934ms/step\n",
      "Epoch 112/200\n",
      "1592/1592 - 1461s - loss: 0.2537 - accuracy: 0.9072 - val_loss: 1.2971 - val_accuracy: 0.6099 - 1461s/epoch - 917ms/step\n",
      "Epoch 113/200\n",
      "1592/1592 - 1472s - loss: 0.2518 - accuracy: 0.9084 - val_loss: 1.3069 - val_accuracy: 0.6095 - 1472s/epoch - 924ms/step\n",
      "Epoch 114/200\n",
      "1592/1592 - 1455s - loss: 0.2501 - accuracy: 0.9088 - val_loss: 1.2840 - val_accuracy: 0.6170 - 1455s/epoch - 914ms/step\n",
      "Epoch 115/200\n",
      "1592/1592 - 1476s - loss: 0.2480 - accuracy: 0.9098 - val_loss: 1.3414 - val_accuracy: 0.6143 - 1476s/epoch - 927ms/step\n",
      "Epoch 116/200\n",
      "1592/1592 - 1464s - loss: 0.2477 - accuracy: 0.9098 - val_loss: 1.3726 - val_accuracy: 0.6102 - 1464s/epoch - 919ms/step\n",
      "Epoch 117/200\n",
      "1592/1592 - 1466s - loss: 0.2466 - accuracy: 0.9101 - val_loss: 1.3271 - val_accuracy: 0.6152 - 1466s/epoch - 921ms/step\n",
      "Epoch 118/200\n",
      "1592/1592 - 1466s - loss: 0.2447 - accuracy: 0.9108 - val_loss: 1.3404 - val_accuracy: 0.6135 - 1466s/epoch - 921ms/step\n",
      "Epoch 119/200\n",
      "1592/1592 - 1472s - loss: 0.2436 - accuracy: 0.9113 - val_loss: 1.2992 - val_accuracy: 0.6161 - 1472s/epoch - 924ms/step\n",
      "Epoch 120/200\n",
      "1592/1592 - 1474s - loss: 0.2411 - accuracy: 0.9124 - val_loss: 1.3402 - val_accuracy: 0.6130 - 1474s/epoch - 926ms/step\n",
      "Epoch 121/200\n",
      "1592/1592 - 1471s - loss: 0.2416 - accuracy: 0.9114 - val_loss: 1.3347 - val_accuracy: 0.6128 - 1471s/epoch - 924ms/step\n",
      "Epoch 122/200\n",
      "1592/1592 - 1489s - loss: 0.2401 - accuracy: 0.9122 - val_loss: 1.3209 - val_accuracy: 0.6183 - 1489s/epoch - 936ms/step\n",
      "Epoch 123/200\n",
      "1592/1592 - 1504s - loss: 0.2392 - accuracy: 0.9131 - val_loss: 1.3574 - val_accuracy: 0.6124 - 1504s/epoch - 945ms/step\n",
      "Epoch 124/200\n",
      "1592/1592 - 1484s - loss: 0.2367 - accuracy: 0.9140 - val_loss: 1.3730 - val_accuracy: 0.6086 - 1484s/epoch - 932ms/step\n",
      "Epoch 125/200\n",
      "1592/1592 - 1488s - loss: 0.2372 - accuracy: 0.9140 - val_loss: 1.3335 - val_accuracy: 0.6161 - 1488s/epoch - 935ms/step\n",
      "Epoch 126/200\n",
      "1592/1592 - 1492s - loss: 0.2365 - accuracy: 0.9138 - val_loss: 1.3577 - val_accuracy: 0.6095 - 1492s/epoch - 937ms/step\n",
      "Epoch 127/200\n",
      "1592/1592 - 1483s - loss: 0.2327 - accuracy: 0.9151 - val_loss: 1.3659 - val_accuracy: 0.6153 - 1483s/epoch - 931ms/step\n",
      "Epoch 128/200\n",
      "1592/1592 - 1482s - loss: 0.2310 - accuracy: 0.9155 - val_loss: 1.4122 - val_accuracy: 0.6138 - 1482s/epoch - 931ms/step\n",
      "Epoch 129/200\n",
      "1592/1592 - 1500s - loss: 0.2322 - accuracy: 0.9157 - val_loss: 1.3297 - val_accuracy: 0.6115 - 1500s/epoch - 942ms/step\n",
      "Epoch 130/200\n",
      "1592/1592 - 1516s - loss: 0.2292 - accuracy: 0.9160 - val_loss: 1.3447 - val_accuracy: 0.6140 - 1516s/epoch - 952ms/step\n",
      "Epoch 131/200\n",
      "1592/1592 - 1494s - loss: 0.2285 - accuracy: 0.9170 - val_loss: 1.3824 - val_accuracy: 0.6159 - 1494s/epoch - 938ms/step\n",
      "Epoch 132/200\n",
      "1592/1592 - 1494s - loss: 0.2281 - accuracy: 0.9168 - val_loss: 1.3766 - val_accuracy: 0.6136 - 1494s/epoch - 938ms/step\n",
      "Epoch 133/200\n",
      "1592/1592 - 1493s - loss: 0.2271 - accuracy: 0.9171 - val_loss: 1.4011 - val_accuracy: 0.6048 - 1493s/epoch - 938ms/step\n",
      "Epoch 134/200\n",
      "1592/1592 - 1478s - loss: 0.2249 - accuracy: 0.9183 - val_loss: 1.3944 - val_accuracy: 0.6160 - 1478s/epoch - 929ms/step\n",
      "Epoch 135/200\n",
      "1592/1592 - 1490s - loss: 0.2262 - accuracy: 0.9173 - val_loss: 1.3592 - val_accuracy: 0.6096 - 1490s/epoch - 936ms/step\n",
      "Epoch 136/200\n",
      "1592/1592 - 1486s - loss: 0.2225 - accuracy: 0.9193 - val_loss: 1.4473 - val_accuracy: 0.6105 - 1486s/epoch - 934ms/step\n",
      "Epoch 137/200\n",
      "1592/1592 - 1495s - loss: 0.2220 - accuracy: 0.9187 - val_loss: 1.3891 - val_accuracy: 0.6158 - 1495s/epoch - 939ms/step\n",
      "Epoch 138/200\n",
      "1592/1592 - 1519s - loss: 0.2207 - accuracy: 0.9192 - val_loss: 1.3995 - val_accuracy: 0.6095 - 1519s/epoch - 954ms/step\n",
      "Epoch 139/200\n",
      "1592/1592 - 1491s - loss: 0.2183 - accuracy: 0.9205 - val_loss: 1.4560 - val_accuracy: 0.6069 - 1491s/epoch - 936ms/step\n",
      "Epoch 140/200\n",
      "1592/1592 - 1500s - loss: 0.2184 - accuracy: 0.9206 - val_loss: 1.3904 - val_accuracy: 0.6083 - 1500s/epoch - 942ms/step\n",
      "Epoch 141/200\n",
      "1592/1592 - 1495s - loss: 0.2182 - accuracy: 0.9201 - val_loss: 1.4331 - val_accuracy: 0.6068 - 1495s/epoch - 939ms/step\n",
      "Epoch 142/200\n",
      "1592/1592 - 1486s - loss: 0.2167 - accuracy: 0.9204 - val_loss: 1.4842 - val_accuracy: 0.6083 - 1486s/epoch - 934ms/step\n",
      "Epoch 143/200\n",
      "1592/1592 - 1490s - loss: 0.2160 - accuracy: 0.9210 - val_loss: 1.3684 - val_accuracy: 0.6092 - 1490s/epoch - 936ms/step\n",
      "Epoch 144/200\n",
      "1592/1592 - 1483s - loss: 0.2144 - accuracy: 0.9217 - val_loss: 1.4176 - val_accuracy: 0.6119 - 1483s/epoch - 932ms/step\n",
      "Epoch 145/200\n",
      "1592/1592 - 1477s - loss: 0.2131 - accuracy: 0.9222 - val_loss: 1.4829 - val_accuracy: 0.6048 - 1477s/epoch - 928ms/step\n",
      "Epoch 146/200\n",
      "1592/1592 - 1479s - loss: 0.2142 - accuracy: 0.9217 - val_loss: 1.3885 - val_accuracy: 0.6169 - 1479s/epoch - 929ms/step\n",
      "Epoch 147/200\n",
      "1592/1592 - 1481s - loss: 0.2106 - accuracy: 0.9231 - val_loss: 1.4422 - val_accuracy: 0.6068 - 1481s/epoch - 930ms/step\n",
      "Epoch 148/200\n",
      "1592/1592 - 1481s - loss: 0.2123 - accuracy: 0.9227 - val_loss: 1.4360 - val_accuracy: 0.6105 - 1481s/epoch - 930ms/step\n",
      "Epoch 149/200\n",
      "1592/1592 - 1464s - loss: 0.2092 - accuracy: 0.9230 - val_loss: 1.4559 - val_accuracy: 0.6131 - 1464s/epoch - 920ms/step\n",
      "Epoch 150/200\n",
      "1592/1592 - 1508s - loss: 0.2077 - accuracy: 0.9239 - val_loss: 1.4799 - val_accuracy: 0.6116 - 1508s/epoch - 947ms/step\n",
      "Epoch 151/200\n",
      "1592/1592 - 1493s - loss: 0.2072 - accuracy: 0.9246 - val_loss: 1.4392 - val_accuracy: 0.6100 - 1493s/epoch - 938ms/step\n",
      "Epoch 152/200\n",
      "1592/1592 - 1486s - loss: 0.2061 - accuracy: 0.9248 - val_loss: 1.4859 - val_accuracy: 0.6098 - 1486s/epoch - 933ms/step\n",
      "Epoch 153/200\n",
      "1592/1592 - 1493s - loss: 0.2063 - accuracy: 0.9243 - val_loss: 1.4227 - val_accuracy: 0.6066 - 1493s/epoch - 938ms/step\n",
      "Epoch 154/200\n",
      "1592/1592 - 1512s - loss: 0.2035 - accuracy: 0.9257 - val_loss: 1.4832 - val_accuracy: 0.6142 - 1512s/epoch - 950ms/step\n",
      "Epoch 155/200\n",
      "1592/1592 - 1521s - loss: 0.2042 - accuracy: 0.9247 - val_loss: 1.4472 - val_accuracy: 0.6122 - 1521s/epoch - 955ms/step\n",
      "Epoch 156/200\n",
      "1592/1592 - 1524s - loss: 0.2017 - accuracy: 0.9261 - val_loss: 1.4677 - val_accuracy: 0.6154 - 1524s/epoch - 957ms/step\n",
      "Epoch 157/200\n",
      "1592/1592 - 1496s - loss: 0.2023 - accuracy: 0.9257 - val_loss: 1.5015 - val_accuracy: 0.6155 - 1496s/epoch - 940ms/step\n",
      "Epoch 158/200\n",
      "1592/1592 - 1503s - loss: 0.2010 - accuracy: 0.9262 - val_loss: 1.4669 - val_accuracy: 0.5977 - 1503s/epoch - 944ms/step\n",
      "Epoch 159/200\n",
      "1592/1592 - 1495s - loss: 0.2007 - accuracy: 0.9267 - val_loss: 1.4705 - val_accuracy: 0.6072 - 1495s/epoch - 939ms/step\n",
      "Epoch 160/200\n",
      "1592/1592 - 1484s - loss: 0.1990 - accuracy: 0.9273 - val_loss: 1.4828 - val_accuracy: 0.6088 - 1484s/epoch - 932ms/step\n",
      "Epoch 161/200\n",
      "1592/1592 - 1496s - loss: 0.1969 - accuracy: 0.9281 - val_loss: 1.4652 - val_accuracy: 0.6109 - 1496s/epoch - 940ms/step\n",
      "Epoch 162/200\n",
      "1592/1592 - 1507s - loss: 0.1976 - accuracy: 0.9275 - val_loss: 1.4786 - val_accuracy: 0.6082 - 1507s/epoch - 946ms/step\n",
      "Epoch 163/200\n",
      "1592/1592 - 1499s - loss: 0.1967 - accuracy: 0.9280 - val_loss: 1.4771 - val_accuracy: 0.6069 - 1499s/epoch - 942ms/step\n",
      "Epoch 164/200\n",
      "1592/1592 - 1510s - loss: 0.1947 - accuracy: 0.9295 - val_loss: 1.4668 - val_accuracy: 0.6075 - 1510s/epoch - 948ms/step\n",
      "Epoch 165/200\n",
      "1592/1592 - 1501s - loss: 0.1949 - accuracy: 0.9286 - val_loss: 1.5111 - val_accuracy: 0.6117 - 1501s/epoch - 943ms/step\n",
      "Epoch 166/200\n",
      "1592/1592 - 1496s - loss: 0.1936 - accuracy: 0.9292 - val_loss: 1.4955 - val_accuracy: 0.6094 - 1496s/epoch - 940ms/step\n",
      "Epoch 167/200\n",
      "1592/1592 - 1498s - loss: 0.1925 - accuracy: 0.9296 - val_loss: 1.5197 - val_accuracy: 0.6063 - 1498s/epoch - 941ms/step\n",
      "Epoch 168/200\n",
      "1592/1592 - 1506s - loss: 0.1909 - accuracy: 0.9303 - val_loss: 1.5141 - val_accuracy: 0.6055 - 1506s/epoch - 946ms/step\n",
      "Epoch 169/200\n",
      "1592/1592 - 1484s - loss: 0.1908 - accuracy: 0.9302 - val_loss: 1.5888 - val_accuracy: 0.6065 - 1484s/epoch - 932ms/step\n",
      "Epoch 170/200\n",
      "1592/1592 - 1491s - loss: 0.1908 - accuracy: 0.9301 - val_loss: 1.5281 - val_accuracy: 0.6074 - 1491s/epoch - 936ms/step\n",
      "Epoch 171/200\n",
      "1592/1592 - 1483s - loss: 0.1893 - accuracy: 0.9307 - val_loss: 1.5255 - val_accuracy: 0.6133 - 1483s/epoch - 932ms/step\n",
      "Epoch 172/200\n",
      "1592/1592 - 1483s - loss: 0.1883 - accuracy: 0.9306 - val_loss: 1.5571 - val_accuracy: 0.6025 - 1483s/epoch - 932ms/step\n",
      "Epoch 173/200\n",
      "1592/1592 - 1495s - loss: 0.1864 - accuracy: 0.9318 - val_loss: 1.5813 - val_accuracy: 0.6079 - 1495s/epoch - 939ms/step\n",
      "Epoch 174/200\n",
      "1592/1592 - 1486s - loss: 0.1869 - accuracy: 0.9314 - val_loss: 1.5495 - val_accuracy: 0.6132 - 1486s/epoch - 934ms/step\n",
      "Epoch 175/200\n",
      "1592/1592 - 1497s - loss: 0.1846 - accuracy: 0.9322 - val_loss: 1.5724 - val_accuracy: 0.6112 - 1497s/epoch - 940ms/step\n",
      "Epoch 176/200\n",
      "1592/1592 - 1490s - loss: 0.1855 - accuracy: 0.9320 - val_loss: 1.5541 - val_accuracy: 0.6050 - 1490s/epoch - 936ms/step\n",
      "Epoch 177/200\n",
      "1592/1592 - 1511s - loss: 0.1841 - accuracy: 0.9323 - val_loss: 1.5232 - val_accuracy: 0.6152 - 1511s/epoch - 949ms/step\n",
      "Epoch 178/200\n",
      "1592/1592 - 1483s - loss: 0.1820 - accuracy: 0.9333 - val_loss: 1.5882 - val_accuracy: 0.6121 - 1483s/epoch - 931ms/step\n",
      "Epoch 179/200\n",
      "1592/1592 - 1488s - loss: 0.1821 - accuracy: 0.9329 - val_loss: 1.5720 - val_accuracy: 0.6076 - 1488s/epoch - 934ms/step\n",
      "Epoch 180/200\n",
      "1592/1592 - 1506s - loss: 0.1801 - accuracy: 0.9343 - val_loss: 1.5843 - val_accuracy: 0.6093 - 1506s/epoch - 946ms/step\n",
      "Epoch 181/200\n",
      "1592/1592 - 1495s - loss: 0.1808 - accuracy: 0.9336 - val_loss: 1.5349 - val_accuracy: 0.6093 - 1495s/epoch - 939ms/step\n",
      "Epoch 182/200\n",
      "1592/1592 - 1495s - loss: 0.1792 - accuracy: 0.9344 - val_loss: 1.5698 - val_accuracy: 0.6047 - 1495s/epoch - 939ms/step\n",
      "Epoch 183/200\n",
      "1592/1592 - 1486s - loss: 0.1787 - accuracy: 0.9338 - val_loss: 1.5817 - val_accuracy: 0.6039 - 1486s/epoch - 934ms/step\n",
      "Epoch 184/200\n",
      "1592/1592 - 1499s - loss: 0.1779 - accuracy: 0.9348 - val_loss: 1.5920 - val_accuracy: 0.6056 - 1499s/epoch - 942ms/step\n",
      "Epoch 185/200\n",
      "1592/1592 - 1497s - loss: 0.1769 - accuracy: 0.9352 - val_loss: 1.5396 - val_accuracy: 0.6119 - 1497s/epoch - 940ms/step\n",
      "Epoch 186/200\n",
      "1592/1592 - 1486s - loss: 0.1759 - accuracy: 0.9354 - val_loss: 1.6018 - val_accuracy: 0.5940 - 1486s/epoch - 933ms/step\n",
      "Epoch 187/200\n",
      "1592/1592 - 1475s - loss: 0.1742 - accuracy: 0.9358 - val_loss: 1.6403 - val_accuracy: 0.6028 - 1475s/epoch - 926ms/step\n",
      "Epoch 188/200\n",
      "1592/1592 - 1491s - loss: 0.1751 - accuracy: 0.9357 - val_loss: 1.6473 - val_accuracy: 0.6091 - 1491s/epoch - 937ms/step\n",
      "Epoch 189/200\n",
      "1592/1592 - 1503s - loss: 0.1730 - accuracy: 0.9364 - val_loss: 1.6526 - val_accuracy: 0.6096 - 1503s/epoch - 944ms/step\n",
      "Epoch 190/200\n",
      "1592/1592 - 1498s - loss: 0.1733 - accuracy: 0.9363 - val_loss: 1.6407 - val_accuracy: 0.6119 - 1498s/epoch - 941ms/step\n",
      "Epoch 191/200\n",
      "1592/1592 - 1551s - loss: 0.1712 - accuracy: 0.9370 - val_loss: 1.6568 - val_accuracy: 0.6029 - 1551s/epoch - 974ms/step\n",
      "Epoch 192/200\n",
      "1592/1592 - 1553s - loss: 0.1711 - accuracy: 0.9372 - val_loss: 1.6320 - val_accuracy: 0.6102 - 1553s/epoch - 975ms/step\n",
      "Epoch 193/200\n",
      "1592/1592 - 1547s - loss: 0.1710 - accuracy: 0.9375 - val_loss: 1.6587 - val_accuracy: 0.6041 - 1547s/epoch - 972ms/step\n",
      "Epoch 194/200\n",
      "1592/1592 - 1562s - loss: 0.1707 - accuracy: 0.9374 - val_loss: 1.6319 - val_accuracy: 0.6070 - 1562s/epoch - 981ms/step\n",
      "Epoch 195/200\n",
      "1592/1592 - 1542s - loss: 0.1685 - accuracy: 0.9381 - val_loss: 1.6313 - val_accuracy: 0.6067 - 1542s/epoch - 969ms/step\n",
      "Epoch 196/200\n",
      "1592/1592 - 1554s - loss: 0.1691 - accuracy: 0.9372 - val_loss: 1.5918 - val_accuracy: 0.6062 - 1554s/epoch - 976ms/step\n",
      "Epoch 197/200\n",
      "1592/1592 - 1523s - loss: 0.1673 - accuracy: 0.9377 - val_loss: 1.6187 - val_accuracy: 0.6101 - 1523s/epoch - 956ms/step\n",
      "Epoch 198/200\n",
      "1592/1592 - 1515s - loss: 0.1670 - accuracy: 0.9389 - val_loss: 1.6183 - val_accuracy: 0.6089 - 1515s/epoch - 952ms/step\n",
      "Epoch 199/200\n",
      "1592/1592 - 1545s - loss: 0.1675 - accuracy: 0.9379 - val_loss: 1.5904 - val_accuracy: 0.6073 - 1545s/epoch - 970ms/step\n",
      "Epoch 200/200\n",
      "1592/1592 - 1544s - loss: 0.1657 - accuracy: 0.9388 - val_loss: 1.6486 - val_accuracy: 0.6098 - 1544s/epoch - 970ms/step\n",
      "CPU times: total: 40d 12h 35min 40s\n",
      "Wall time: 3d 12h 4min 16s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2431a4b12d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='auto',\n",
    "    save_best_only=True)\n",
    "\n",
    "deeplob_encoder_decoder.fit(trainX_CNN, trainY_CNN, validation_data=(valX_CNN, valY_CNN),\n",
    "            epochs=200, batch_size=128, verbose=2, callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w-26dzYHLek4",
    "outputId": "0e148183-d701-4cdc-f40f-9dbad91988c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4359/4359 [==============================] - 307s 65ms/step\n"
     ]
    }
   ],
   "source": [
    "deeplob_encoder_decoder.load_weights(checkpoint_filepath)\n",
    "pred = deeplob_encoder_decoder.predict(testX_CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XX2UQWkJLizk",
    "outputId": "b8fa0be3-4149-4d72-8427-b28b8c03a59a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: 0.7290806377609543\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7066    0.7224    0.7144     47915\n",
      "           1     0.8354    0.7116    0.7686     48050\n",
      "           2     0.6635    0.7557    0.7066     43523\n",
      "\n",
      "    accuracy                         0.7291    139488\n",
      "   macro avg     0.7352    0.7299    0.7299    139488\n",
      "weighted avg     0.7375    0.7291    0.7306    139488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('accuracy_score:', accuracy_score(np.argmax(testY_CNN, axis=1), np.argmax(pred, axis=1)))\n",
    "print(classification_report(np.argmax(testY_CNN, axis=1), np.argmax(pred, axis=1), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "qkha5y5ILSi6",
    "outputId": "09f32ecd-b2ed-49fb-d469-1fed99278779"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "c = confusion_matrix(np.argmax(testY_CNN, axis=1), np.argmax(pred, axis=1), normalize=\"true\")\n",
    "disp = ConfusionMatrixDisplay(c)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
