{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M9qS_jUeA0l4",
    "outputId": "5a5bcad3-dd11-4373-e136-d023a736170a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149, 254750)\n",
      "(149, 203800)\n",
      "(149, 50950)\n",
      "(149, 139587)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "dec_data = np.loadtxt('./Train_Dst_NoAuction_DecPre_CF_7.txt')\n",
    "dec_train = dec_data[:, :int(np.floor(dec_data.shape[1] * 0.8))]\n",
    "dec_val = dec_data[:, int(np.floor(dec_data.shape[1] * 0.8)):]\n",
    "\n",
    "dec_test1 = np.loadtxt('./Test_Dst_NoAuction_DecPre_CF_7.txt')\n",
    "dec_test2 = np.loadtxt('./Test_Dst_NoAuction_DecPre_CF_8.txt')\n",
    "dec_test3 = np.loadtxt('./Test_Dst_NoAuction_DecPre_CF_9.txt')\n",
    "dec_test = np.hstack((dec_test1, dec_test2, dec_test3))\n",
    "\n",
    "print(dec_data.shape)\n",
    "print(dec_train.shape)\n",
    "print(dec_val.shape)\n",
    "print(dec_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QoQe5U4yBOTP"
   },
   "outputs": [],
   "source": [
    "# load packages\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.models import load_model, Model\n",
    "from keras.layers import Flatten, Dense, Dropout, Activation, Input, LSTM, Reshape, Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import LeakyReLU\n",
    "# from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.utils import to_categorical\n",
    "# from keras.utils import np_utils\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set random seeds\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5a1ZNnSjBQ7q"
   },
   "outputs": [],
   "source": [
    "def prepare_x(data):\n",
    "    df1 = data[:40, :].T\n",
    "    return np.array(df1)\n",
    "\n",
    "def get_label(data):\n",
    "    lob = data[-5:, :].T\n",
    "    return lob\n",
    "\n",
    "def data_classification(X, Y, T):\n",
    "    [N, D] = X.shape\n",
    "    df = np.array(X)\n",
    "    dY = np.array(Y)\n",
    "    dataY = dY[T - 1:N]\n",
    "    dataX = np.zeros((N - T + 1, T, D))\n",
    "    for i in range(T, N + 1):\n",
    "        dataX[i - T] = df[i - T:i, :]\n",
    "    return dataX.reshape(dataX.shape + (1,)), dataY\n",
    "\n",
    "def prepare_x_y(data, k, T):\n",
    "    x = prepare_x(data)\n",
    "    y = get_label(data)\n",
    "    x, y = data_classification(x, y, T=T)\n",
    "    y = y[:,k] - 1\n",
    "    y = to_categorical(y, 3)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "r4FvlMRHGm97"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.W = self.add_weight(name='attention_weight',\n",
    "                                 shape=(input_shape[-1], input_shape[-1]),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        # Calculate attention scores\n",
    "        e = tf.keras.backend.dot(x, self.W)\n",
    "\n",
    "        # Calculate attention weights using softmax activation\n",
    "        a = tf.keras.activations.softmax(e, axis=-2)\n",
    "\n",
    "        # Apply attention weights to the input sequence\n",
    "        output = x * a\n",
    "\n",
    "        # Sum the weighted inputs to get the attention output\n",
    "        output = tf.keras.backend.sum(output, axis=-2)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gE7CLstKBSpr"
   },
   "outputs": [],
   "source": [
    "k = 4 # which prediction horizon\n",
    "T = 100 # the length of a single input i. e lookback_timestep THIS HYPERPARAMETER IS FINE TUNED FOR MODEL CONVERGENCE\n",
    "n_hiddens = 64\n",
    "checkpoint_filepath = './BiADLOBmodel_tensorflow2/weights'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NubSWmNjBWD4",
    "outputId": "b4f23663-2ac5-4abc-805b-0e7c60ac1696"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149, 203800)\n",
      "(203701, 100, 40, 1) (203701, 3)\n"
     ]
    }
   ],
   "source": [
    "# # Training Dataset Shrinking\n",
    "# dec_train = dec_train[:,:50000]\n",
    "print(dec_train.shape)\n",
    "trainX_CNN, trainY_CNN = prepare_x_y(dec_train, k, T)\n",
    "print(trainX_CNN.shape, trainY_CNN.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bI2kVMq3BYOe",
    "outputId": "ea4e1bb8-4703-4535-d370-9036b28b2928"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149, 139587)\n",
      "(139488, 100, 40, 1) (139488, 3)\n"
     ]
    }
   ],
   "source": [
    "# Testing Dataset Shrinking\n",
    "# dec_test = dec_test[:,:20000]\n",
    "print(dec_test.shape)\n",
    "testX_CNN, testY_CNN = prepare_x_y(dec_test, k, T)\n",
    "print(testX_CNN.shape, testY_CNN.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d8WN89lWBZ1p",
    "outputId": "0dea99a9-3c4c-4b37-e8e1-06c2828c180a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149, 50950)\n",
      "(50851, 100, 40, 1) (50851, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Validation Dataset Shrinking\n",
    "# dec_val = dec_val[:,:10000]\n",
    "print(dec_val.shape)\n",
    "valX_CNN, valY_CNN = prepare_x_y(dec_val, k, T)\n",
    "print(valX_CNN.shape, valY_CNN.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_vUIbc0zBbvk",
    "outputId": "af2fb026-9305-4396-8591-d12650e78f9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(203701, 100, 40, 1) (203701, 3)\n",
      "(50851, 100, 40, 1) (50851, 3)\n",
      "(139488, 100, 40, 1) (139488, 3)\n"
     ]
    }
   ],
   "source": [
    "print(trainX_CNN.shape, trainY_CNN.shape)\n",
    "print(valX_CNN.shape, valY_CNN.shape)\n",
    "print(testX_CNN.shape, testY_CNN.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k3dH8BwbBdlu",
    "outputId": "a179c10b-8375-49fa-a138-a4c288a5c088"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 100, 40, 1)]         0         []                            \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 100, 20, 32)          96        ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)     (None, 100, 20, 32)          0         ['conv2d[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 100, 20, 32)          4128      ['leaky_re_lu[0][0]']         \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 100, 20, 32)          0         ['conv2d_1[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 100, 20, 32)          4128      ['leaky_re_lu_1[0][0]']       \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 100, 20, 32)          0         ['conv2d_2[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 100, 10, 32)          2080      ['leaky_re_lu_2[0][0]']       \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 100, 10, 32)          0         ['conv2d_3[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, 100, 10, 32)          4128      ['leaky_re_lu_3[0][0]']       \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 100, 10, 32)          0         ['conv2d_4[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)           (None, 100, 10, 32)          4128      ['leaky_re_lu_4[0][0]']       \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 100, 10, 32)          0         ['conv2d_5[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)           (None, 100, 1, 32)           10272     ['leaky_re_lu_5[0][0]']       \n",
      "                                                                                                  \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 100, 1, 32)           0         ['conv2d_6[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)           (None, 100, 1, 32)           4128      ['leaky_re_lu_6[0][0]']       \n",
      "                                                                                                  \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, 100, 1, 32)           0         ['conv2d_7[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)           (None, 100, 1, 32)           4128      ['leaky_re_lu_7[0][0]']       \n",
      "                                                                                                  \n",
      " leaky_re_lu_8 (LeakyReLU)   (None, 100, 1, 32)           0         ['conv2d_8[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)           (None, 100, 1, 64)           2112      ['leaky_re_lu_8[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)          (None, 100, 1, 64)           2112      ['leaky_re_lu_8[0][0]']       \n",
      "                                                                                                  \n",
      " leaky_re_lu_9 (LeakyReLU)   (None, 100, 1, 64)           0         ['conv2d_9[0][0]']            \n",
      "                                                                                                  \n",
      " leaky_re_lu_11 (LeakyReLU)  (None, 100, 1, 64)           0         ['conv2d_11[0][0]']           \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2  (None, 100, 1, 32)           0         ['leaky_re_lu_8[0][0]']       \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)          (None, 100, 1, 64)           12352     ['leaky_re_lu_9[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)          (None, 100, 1, 64)           20544     ['leaky_re_lu_11[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)          (None, 100, 1, 64)           2112      ['max_pooling2d[0][0]']       \n",
      "                                                                                                  \n",
      " leaky_re_lu_10 (LeakyReLU)  (None, 100, 1, 64)           0         ['conv2d_10[0][0]']           \n",
      "                                                                                                  \n",
      " leaky_re_lu_12 (LeakyReLU)  (None, 100, 1, 64)           0         ['conv2d_12[0][0]']           \n",
      "                                                                                                  \n",
      " leaky_re_lu_13 (LeakyReLU)  (None, 100, 1, 64)           0         ['conv2d_13[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 100, 1, 192)          0         ['leaky_re_lu_10[0][0]',      \n",
      "                                                                     'leaky_re_lu_12[0][0]',      \n",
      "                                                                     'leaky_re_lu_13[0][0]']      \n",
      "                                                                                                  \n",
      " reshape (Reshape)           (None, 100, 192)             0         ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 100, 192)             0         ['reshape[0][0]']             \n",
      "                                                                                                  \n",
      " bidirectional (Bidirection  (None, 100, 128)             131584    ['dropout[0][0]']             \n",
      " al)                                                                                              \n",
      "                                                                                                  \n",
      " attention_layer (Attention  (None, 128)                  16384     ['bidirectional[0][0]']       \n",
      " Layer)                                                                                           \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 3)                    387       ['attention_layer[0][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 224803 (878.14 KB)\n",
      "Trainable params: 224803 (878.14 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional\n",
    "def create_biadeeplob(T, NF, number_of_lstm):\n",
    "    input_lmd = Input(shape=(T, NF, 1))\n",
    "\n",
    "    # build the convolutional block\n",
    "    conv_first1 = Conv2D(32, (1, 2), strides=(1, 2))(input_lmd)\n",
    "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    conv_first1 = Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
    "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    conv_first1 = Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
    "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
    "\n",
    "    conv_first1 = Conv2D(32, (1, 2), strides=(1, 2))(conv_first1)\n",
    "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    conv_first1 = Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
    "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    conv_first1 = Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
    "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
    "\n",
    "    conv_first1 = Conv2D(32, (1, 10))(conv_first1)\n",
    "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    conv_first1 = Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
    "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
    "    conv_first1 = Conv2D(32, (4, 1), padding='same')(conv_first1)\n",
    "    conv_first1 = keras.layers.LeakyReLU(alpha=0.01)(conv_first1)\n",
    "\n",
    "    # build the inception module\n",
    "    convsecond_1 = Conv2D(64, (1, 1), padding='same')(conv_first1)\n",
    "    convsecond_1 = keras.layers.LeakyReLU(alpha=0.01)(convsecond_1)\n",
    "    convsecond_1 = Conv2D(64, (3, 1), padding='same')(convsecond_1)\n",
    "    convsecond_1 = keras.layers.LeakyReLU(alpha=0.01)(convsecond_1)\n",
    "\n",
    "    convsecond_2 = Conv2D(64, (1, 1), padding='same')(conv_first1)\n",
    "    convsecond_2 = keras.layers.LeakyReLU(alpha=0.01)(convsecond_2)\n",
    "    convsecond_2 = Conv2D(64, (5, 1), padding='same')(convsecond_2)\n",
    "    convsecond_2 = keras.layers.LeakyReLU(alpha=0.01)(convsecond_2)\n",
    "\n",
    "    convsecond_3 = MaxPooling2D((3, 1), strides=(1, 1), padding='same')(conv_first1)\n",
    "    convsecond_3 = Conv2D(64, (1, 1), padding='same')(convsecond_3)\n",
    "    convsecond_3 = keras.layers.LeakyReLU(alpha=0.01)(convsecond_3)\n",
    "\n",
    "    convsecond_output = keras.layers.concatenate([convsecond_1, convsecond_2, convsecond_3], axis=3)\n",
    "    conv_reshape = Reshape((int(convsecond_output.shape[1]), int(convsecond_output.shape[3])))(convsecond_output)\n",
    "    conv_reshape = keras.layers.Dropout(0.2, noise_shape=(None, 1, int(conv_reshape.shape[2])))(conv_reshape, training=True)\n",
    "\n",
    "    # build the last LSTM layer\n",
    "    conv_bi_lstm = Bidirectional(LSTM(number_of_lstm, return_sequences=True))(conv_reshape)\n",
    "\n",
    "    # add an attention layer\n",
    "    attention_output = AttentionLayer()(conv_bi_lstm)\n",
    "\n",
    "    # build the output layer\n",
    "    out = Dense(3, activation='softmax')(attention_output)\n",
    "\n",
    "    model = Model(inputs=input_lmd, outputs=out)\n",
    "    adam = keras.optimizers.Adam(lr=0.0001)\n",
    "    model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "biadeeplob = create_biadeeplob(trainX_CNN.shape[1], trainX_CNN.shape[2], n_hiddens)\n",
    "biadeeplob.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9fbF-YRWBgG7",
    "outputId": "5d15a367-a241-4961-99bc-9b61aaf74d81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1592/1592 - 1224s - loss: 1.0240 - accuracy: 0.4196 - val_loss: 1.0867 - val_accuracy: 0.3723 - 1224s/epoch - 769ms/step\n",
      "Epoch 2/200\n",
      "1592/1592 - 1162s - loss: 1.0142 - accuracy: 0.4311 - val_loss: 1.1212 - val_accuracy: 0.3712 - 1162s/epoch - 730ms/step\n",
      "Epoch 3/200\n",
      "1592/1592 - 1177s - loss: 0.9382 - accuracy: 0.5094 - val_loss: 0.9983 - val_accuracy: 0.4895 - 1177s/epoch - 739ms/step\n",
      "Epoch 4/200\n",
      "1592/1592 - 1170s - loss: 0.8199 - accuracy: 0.6075 - val_loss: 0.9082 - val_accuracy: 0.5649 - 1170s/epoch - 735ms/step\n",
      "Epoch 5/200\n",
      "1592/1592 - 1173s - loss: 0.6426 - accuracy: 0.7275 - val_loss: 0.8265 - val_accuracy: 0.6233 - 1173s/epoch - 737ms/step\n",
      "Epoch 6/200\n",
      "1592/1592 - 1192s - loss: 0.5486 - accuracy: 0.7799 - val_loss: 0.7782 - val_accuracy: 0.6631 - 1192s/epoch - 749ms/step\n",
      "Epoch 7/200\n",
      "1592/1592 - 1191s - loss: 0.4943 - accuracy: 0.8071 - val_loss: 0.7862 - val_accuracy: 0.6765 - 1191s/epoch - 748ms/step\n",
      "Epoch 8/200\n",
      "1592/1592 - 1195s - loss: 0.4553 - accuracy: 0.8249 - val_loss: 0.7665 - val_accuracy: 0.6754 - 1195s/epoch - 751ms/step\n",
      "Epoch 9/200\n",
      "1592/1592 - 1223s - loss: 0.4256 - accuracy: 0.8381 - val_loss: 0.7929 - val_accuracy: 0.6743 - 1223s/epoch - 768ms/step\n",
      "Epoch 10/200\n",
      "1592/1592 - 1231s - loss: 0.4031 - accuracy: 0.8484 - val_loss: 0.8105 - val_accuracy: 0.6732 - 1231s/epoch - 773ms/step\n",
      "Epoch 11/200\n",
      "1592/1592 - 1213s - loss: 0.3818 - accuracy: 0.8581 - val_loss: 0.8416 - val_accuracy: 0.6667 - 1213s/epoch - 762ms/step\n",
      "Epoch 12/200\n",
      "1592/1592 - 1213s - loss: 0.3662 - accuracy: 0.8641 - val_loss: 0.8723 - val_accuracy: 0.6690 - 1213s/epoch - 762ms/step\n",
      "Epoch 13/200\n",
      "1592/1592 - 1241s - loss: 0.3538 - accuracy: 0.8687 - val_loss: 0.8912 - val_accuracy: 0.6598 - 1241s/epoch - 780ms/step\n",
      "Epoch 14/200\n",
      "1592/1592 - 1235s - loss: 0.3442 - accuracy: 0.8724 - val_loss: 0.8717 - val_accuracy: 0.6631 - 1235s/epoch - 776ms/step\n",
      "Epoch 15/200\n",
      "1592/1592 - 1212s - loss: 0.3317 - accuracy: 0.8769 - val_loss: 0.9544 - val_accuracy: 0.6472 - 1212s/epoch - 762ms/step\n",
      "Epoch 16/200\n",
      "1592/1592 - 1223s - loss: 0.3220 - accuracy: 0.8812 - val_loss: 0.9170 - val_accuracy: 0.6580 - 1223s/epoch - 768ms/step\n",
      "Epoch 17/200\n",
      "1592/1592 - 1232s - loss: 0.3153 - accuracy: 0.8842 - val_loss: 0.9288 - val_accuracy: 0.6615 - 1232s/epoch - 774ms/step\n",
      "Epoch 18/200\n",
      "1592/1592 - 1241s - loss: 0.3063 - accuracy: 0.8877 - val_loss: 0.9456 - val_accuracy: 0.6541 - 1241s/epoch - 779ms/step\n",
      "Epoch 19/200\n",
      "1592/1592 - 1240s - loss: 0.3010 - accuracy: 0.8888 - val_loss: 0.9593 - val_accuracy: 0.6590 - 1240s/epoch - 779ms/step\n",
      "Epoch 20/200\n",
      "1592/1592 - 1218s - loss: 0.2967 - accuracy: 0.8909 - val_loss: 0.9929 - val_accuracy: 0.6576 - 1218s/epoch - 765ms/step\n",
      "Epoch 21/200\n",
      "1592/1592 - 1210s - loss: 0.2912 - accuracy: 0.8932 - val_loss: 0.9448 - val_accuracy: 0.6642 - 1210s/epoch - 760ms/step\n",
      "Epoch 22/200\n",
      "1592/1592 - 1218s - loss: 0.2929 - accuracy: 0.8924 - val_loss: 0.9712 - val_accuracy: 0.6590 - 1218s/epoch - 765ms/step\n",
      "Epoch 23/200\n",
      "1592/1592 - 1209s - loss: 0.2819 - accuracy: 0.8965 - val_loss: 0.9960 - val_accuracy: 0.6597 - 1209s/epoch - 759ms/step\n",
      "Epoch 24/200\n",
      "1592/1592 - 1203s - loss: 0.2786 - accuracy: 0.8982 - val_loss: 0.9948 - val_accuracy: 0.6608 - 1203s/epoch - 756ms/step\n",
      "Epoch 25/200\n",
      "1592/1592 - 1213s - loss: 0.2746 - accuracy: 0.8992 - val_loss: 0.9960 - val_accuracy: 0.6559 - 1213s/epoch - 762ms/step\n",
      "Epoch 26/200\n",
      "1592/1592 - 1219s - loss: 0.2751 - accuracy: 0.8992 - val_loss: 1.0020 - val_accuracy: 0.6597 - 1219s/epoch - 766ms/step\n",
      "Epoch 27/200\n",
      "1592/1592 - 1213s - loss: 0.2695 - accuracy: 0.9014 - val_loss: 1.0161 - val_accuracy: 0.6526 - 1213s/epoch - 762ms/step\n",
      "Epoch 28/200\n",
      "1592/1592 - 1212s - loss: 0.2673 - accuracy: 0.9017 - val_loss: 1.0407 - val_accuracy: 0.6534 - 1212s/epoch - 761ms/step\n",
      "Epoch 29/200\n",
      "1592/1592 - 1206s - loss: 0.2616 - accuracy: 0.9038 - val_loss: 1.0368 - val_accuracy: 0.6549 - 1206s/epoch - 757ms/step\n",
      "Epoch 30/200\n",
      "1592/1592 - 1204s - loss: 0.2605 - accuracy: 0.9037 - val_loss: 1.0648 - val_accuracy: 0.6481 - 1204s/epoch - 756ms/step\n",
      "Epoch 31/200\n",
      "1592/1592 - 1202s - loss: 0.2638 - accuracy: 0.9031 - val_loss: 1.0028 - val_accuracy: 0.6573 - 1202s/epoch - 755ms/step\n",
      "Epoch 32/200\n",
      "1592/1592 - 1206s - loss: 0.2575 - accuracy: 0.9052 - val_loss: 1.0488 - val_accuracy: 0.6586 - 1206s/epoch - 757ms/step\n",
      "Epoch 33/200\n",
      "1592/1592 - 1195s - loss: 0.2523 - accuracy: 0.9071 - val_loss: 1.0672 - val_accuracy: 0.6546 - 1195s/epoch - 751ms/step\n",
      "Epoch 34/200\n",
      "1592/1592 - 1206s - loss: 0.2511 - accuracy: 0.9079 - val_loss: 1.0429 - val_accuracy: 0.6572 - 1206s/epoch - 757ms/step\n",
      "Epoch 35/200\n",
      "1592/1592 - 1203s - loss: 0.2505 - accuracy: 0.9079 - val_loss: 1.0694 - val_accuracy: 0.6587 - 1203s/epoch - 756ms/step\n",
      "Epoch 36/200\n",
      "1592/1592 - 1193s - loss: 0.2474 - accuracy: 0.9090 - val_loss: 1.1123 - val_accuracy: 0.6546 - 1193s/epoch - 749ms/step\n",
      "Epoch 37/200\n",
      "1592/1592 - 1193s - loss: 0.2460 - accuracy: 0.9093 - val_loss: 1.0838 - val_accuracy: 0.6537 - 1193s/epoch - 749ms/step\n",
      "Epoch 38/200\n",
      "1592/1592 - 1191s - loss: 0.2620 - accuracy: 0.9032 - val_loss: 1.1088 - val_accuracy: 0.6433 - 1191s/epoch - 748ms/step\n",
      "Epoch 39/200\n",
      "1592/1592 - 1188s - loss: 0.2420 - accuracy: 0.9107 - val_loss: 1.1117 - val_accuracy: 0.6425 - 1188s/epoch - 746ms/step\n",
      "Epoch 40/200\n",
      "1592/1592 - 1200s - loss: 0.2389 - accuracy: 0.9121 - val_loss: 1.0747 - val_accuracy: 0.6556 - 1200s/epoch - 754ms/step\n",
      "Epoch 41/200\n",
      "1592/1592 - 1195s - loss: 0.2385 - accuracy: 0.9117 - val_loss: 1.0865 - val_accuracy: 0.6488 - 1195s/epoch - 750ms/step\n",
      "Epoch 42/200\n",
      "1592/1592 - 1181s - loss: 0.2361 - accuracy: 0.9126 - val_loss: 1.0690 - val_accuracy: 0.6545 - 1181s/epoch - 742ms/step\n",
      "Epoch 43/200\n",
      "1592/1592 - 1192s - loss: 0.2366 - accuracy: 0.9124 - val_loss: 1.1312 - val_accuracy: 0.6531 - 1192s/epoch - 749ms/step\n",
      "Epoch 44/200\n",
      "1592/1592 - 1188s - loss: 0.2333 - accuracy: 0.9133 - val_loss: 1.1164 - val_accuracy: 0.6508 - 1188s/epoch - 746ms/step\n",
      "Epoch 45/200\n",
      "1592/1592 - 1190s - loss: 0.2360 - accuracy: 0.9129 - val_loss: 1.1160 - val_accuracy: 0.6536 - 1190s/epoch - 747ms/step\n",
      "Epoch 46/200\n",
      "1592/1592 - 1193s - loss: 0.2336 - accuracy: 0.9136 - val_loss: 1.1355 - val_accuracy: 0.6424 - 1193s/epoch - 749ms/step\n",
      "Epoch 47/200\n",
      "1592/1592 - 1191s - loss: 0.2289 - accuracy: 0.9154 - val_loss: 1.1238 - val_accuracy: 0.6471 - 1191s/epoch - 748ms/step\n",
      "Epoch 48/200\n",
      "1592/1592 - 1184s - loss: 0.2286 - accuracy: 0.9150 - val_loss: 1.1485 - val_accuracy: 0.6477 - 1184s/epoch - 744ms/step\n",
      "Epoch 49/200\n",
      "1592/1592 - 1186s - loss: 0.2288 - accuracy: 0.9148 - val_loss: 1.1091 - val_accuracy: 0.6520 - 1186s/epoch - 745ms/step\n",
      "Epoch 50/200\n",
      "1592/1592 - 1192s - loss: 0.2268 - accuracy: 0.9160 - val_loss: 1.1687 - val_accuracy: 0.6503 - 1192s/epoch - 749ms/step\n",
      "Epoch 51/200\n",
      "1592/1592 - 1184s - loss: 0.2239 - accuracy: 0.9172 - val_loss: 1.1309 - val_accuracy: 0.6524 - 1184s/epoch - 744ms/step\n",
      "Epoch 52/200\n",
      "1592/1592 - 1186s - loss: 0.2237 - accuracy: 0.9174 - val_loss: 1.1618 - val_accuracy: 0.6500 - 1186s/epoch - 745ms/step\n",
      "Epoch 53/200\n",
      "1592/1592 - 1191s - loss: 0.2236 - accuracy: 0.9175 - val_loss: 1.1203 - val_accuracy: 0.6513 - 1191s/epoch - 748ms/step\n",
      "Epoch 54/200\n",
      "1592/1592 - 1191s - loss: 0.2243 - accuracy: 0.9165 - val_loss: 1.1544 - val_accuracy: 0.6360 - 1191s/epoch - 748ms/step\n",
      "Epoch 55/200\n",
      "1592/1592 - 1196s - loss: 0.2222 - accuracy: 0.9176 - val_loss: 1.1648 - val_accuracy: 0.6404 - 1196s/epoch - 751ms/step\n",
      "Epoch 56/200\n",
      "1592/1592 - 1191s - loss: 0.2207 - accuracy: 0.9183 - val_loss: 1.1816 - val_accuracy: 0.6464 - 1191s/epoch - 748ms/step\n",
      "Epoch 57/200\n",
      "1592/1592 - 1196s - loss: 0.2197 - accuracy: 0.9186 - val_loss: 1.1594 - val_accuracy: 0.6512 - 1196s/epoch - 751ms/step\n",
      "Epoch 58/200\n",
      "1592/1592 - 1197s - loss: 0.2203 - accuracy: 0.9185 - val_loss: 1.1493 - val_accuracy: 0.6539 - 1197s/epoch - 752ms/step\n",
      "Epoch 59/200\n",
      "1592/1592 - 1192s - loss: 0.2152 - accuracy: 0.9203 - val_loss: 1.1211 - val_accuracy: 0.6525 - 1192s/epoch - 749ms/step\n",
      "Epoch 60/200\n",
      "1592/1592 - 1185s - loss: 0.2170 - accuracy: 0.9199 - val_loss: 1.1866 - val_accuracy: 0.6488 - 1185s/epoch - 744ms/step\n",
      "Epoch 61/200\n",
      "1592/1592 - 1199s - loss: 0.2160 - accuracy: 0.9199 - val_loss: 1.1876 - val_accuracy: 0.6526 - 1199s/epoch - 753ms/step\n",
      "Epoch 62/200\n",
      "1592/1592 - 1190s - loss: 0.2169 - accuracy: 0.9192 - val_loss: 1.1903 - val_accuracy: 0.6440 - 1190s/epoch - 748ms/step\n",
      "Epoch 63/200\n",
      "1592/1592 - 1187s - loss: 0.2168 - accuracy: 0.9198 - val_loss: 1.1587 - val_accuracy: 0.6505 - 1187s/epoch - 746ms/step\n",
      "Epoch 64/200\n",
      "1592/1592 - 1183s - loss: 0.2114 - accuracy: 0.9213 - val_loss: 1.1695 - val_accuracy: 0.6521 - 1183s/epoch - 743ms/step\n",
      "Epoch 65/200\n",
      "1592/1592 - 1185s - loss: 0.2105 - accuracy: 0.9217 - val_loss: 1.1742 - val_accuracy: 0.6536 - 1185s/epoch - 744ms/step\n",
      "Epoch 66/200\n",
      "1592/1592 - 1174s - loss: 0.2126 - accuracy: 0.9210 - val_loss: 1.2077 - val_accuracy: 0.6574 - 1174s/epoch - 737ms/step\n",
      "Epoch 67/200\n",
      "1592/1592 - 1183s - loss: 0.2077 - accuracy: 0.9229 - val_loss: 1.2176 - val_accuracy: 0.6517 - 1183s/epoch - 743ms/step\n",
      "Epoch 68/200\n",
      "1592/1592 - 1182s - loss: 0.2119 - accuracy: 0.9213 - val_loss: 1.1864 - val_accuracy: 0.6509 - 1182s/epoch - 742ms/step\n",
      "Epoch 69/200\n",
      "1592/1592 - 1184s - loss: 0.2092 - accuracy: 0.9221 - val_loss: 1.2098 - val_accuracy: 0.6463 - 1184s/epoch - 744ms/step\n",
      "Epoch 70/200\n",
      "1592/1592 - 1200s - loss: 0.2069 - accuracy: 0.9237 - val_loss: 1.1575 - val_accuracy: 0.6494 - 1200s/epoch - 754ms/step\n",
      "Epoch 71/200\n",
      "1592/1592 - 1203s - loss: 0.2067 - accuracy: 0.9233 - val_loss: 1.2590 - val_accuracy: 0.6406 - 1203s/epoch - 756ms/step\n",
      "Epoch 72/200\n",
      "1592/1592 - 1208s - loss: 0.2076 - accuracy: 0.9225 - val_loss: 1.2225 - val_accuracy: 0.6411 - 1208s/epoch - 759ms/step\n",
      "Epoch 73/200\n",
      "1592/1592 - 1207s - loss: 0.2120 - accuracy: 0.9212 - val_loss: 1.1862 - val_accuracy: 0.6506 - 1207s/epoch - 758ms/step\n",
      "Epoch 74/200\n",
      "1592/1592 - 1198s - loss: 0.2043 - accuracy: 0.9239 - val_loss: 1.2044 - val_accuracy: 0.6549 - 1198s/epoch - 753ms/step\n",
      "Epoch 75/200\n",
      "1592/1592 - 1200s - loss: 0.2047 - accuracy: 0.9229 - val_loss: 1.2150 - val_accuracy: 0.6476 - 1200s/epoch - 754ms/step\n",
      "Epoch 76/200\n",
      "1592/1592 - 1194s - loss: 0.2051 - accuracy: 0.9235 - val_loss: 1.1969 - val_accuracy: 0.6532 - 1194s/epoch - 750ms/step\n",
      "Epoch 77/200\n",
      "1592/1592 - 1198s - loss: 0.2025 - accuracy: 0.9247 - val_loss: 1.2499 - val_accuracy: 0.6353 - 1198s/epoch - 753ms/step\n",
      "Epoch 78/200\n",
      "1592/1592 - 1196s - loss: 0.2034 - accuracy: 0.9243 - val_loss: 1.2161 - val_accuracy: 0.6508 - 1196s/epoch - 751ms/step\n",
      "Epoch 79/200\n",
      "1592/1592 - 1201s - loss: 0.2032 - accuracy: 0.9243 - val_loss: 1.1811 - val_accuracy: 0.6509 - 1201s/epoch - 754ms/step\n",
      "Epoch 80/200\n",
      "1592/1592 - 1205s - loss: 0.2006 - accuracy: 0.9249 - val_loss: 1.2349 - val_accuracy: 0.6435 - 1205s/epoch - 757ms/step\n",
      "Epoch 81/200\n",
      "1592/1592 - 1215s - loss: 0.2051 - accuracy: 0.9234 - val_loss: 1.2109 - val_accuracy: 0.6501 - 1215s/epoch - 763ms/step\n",
      "Epoch 82/200\n",
      "1592/1592 - 1193s - loss: 0.1994 - accuracy: 0.9256 - val_loss: 1.2207 - val_accuracy: 0.6528 - 1193s/epoch - 749ms/step\n",
      "Epoch 83/200\n",
      "1592/1592 - 1187s - loss: 0.1990 - accuracy: 0.9263 - val_loss: 1.2413 - val_accuracy: 0.6493 - 1187s/epoch - 745ms/step\n",
      "Epoch 84/200\n",
      "1592/1592 - 1195s - loss: 0.2000 - accuracy: 0.9257 - val_loss: 1.2393 - val_accuracy: 0.6496 - 1195s/epoch - 751ms/step\n",
      "Epoch 85/200\n",
      "1592/1592 - 1210s - loss: 0.2001 - accuracy: 0.9253 - val_loss: 1.2204 - val_accuracy: 0.6533 - 1210s/epoch - 760ms/step\n",
      "Epoch 86/200\n",
      "1592/1592 - 1213s - loss: 0.1976 - accuracy: 0.9266 - val_loss: 1.2138 - val_accuracy: 0.6501 - 1213s/epoch - 762ms/step\n",
      "Epoch 87/200\n",
      "1592/1592 - 1202s - loss: 0.2151 - accuracy: 0.9201 - val_loss: 1.1114 - val_accuracy: 0.6374 - 1202s/epoch - 755ms/step\n",
      "Epoch 88/200\n",
      "1592/1592 - 1209s - loss: 0.2092 - accuracy: 0.9223 - val_loss: 1.2495 - val_accuracy: 0.6449 - 1209s/epoch - 759ms/step\n",
      "Epoch 89/200\n",
      "1592/1592 - 1211s - loss: 0.1973 - accuracy: 0.9262 - val_loss: 1.2136 - val_accuracy: 0.6541 - 1211s/epoch - 761ms/step\n",
      "Epoch 90/200\n",
      "1592/1592 - 1211s - loss: 0.2040 - accuracy: 0.9239 - val_loss: 1.2068 - val_accuracy: 0.6494 - 1211s/epoch - 761ms/step\n",
      "Epoch 91/200\n",
      "1592/1592 - 1202s - loss: 0.1926 - accuracy: 0.9281 - val_loss: 1.2010 - val_accuracy: 0.6521 - 1202s/epoch - 755ms/step\n",
      "Epoch 92/200\n",
      "1592/1592 - 1212s - loss: 0.1948 - accuracy: 0.9272 - val_loss: 1.2394 - val_accuracy: 0.6495 - 1212s/epoch - 761ms/step\n",
      "Epoch 93/200\n",
      "1592/1592 - 1206s - loss: 0.2204 - accuracy: 0.9175 - val_loss: 1.1249 - val_accuracy: 0.6508 - 1206s/epoch - 757ms/step\n",
      "Epoch 94/200\n",
      "1592/1592 - 1214s - loss: 0.1972 - accuracy: 0.9260 - val_loss: 1.2349 - val_accuracy: 0.6492 - 1214s/epoch - 763ms/step\n",
      "Epoch 95/200\n",
      "1592/1592 - 1204s - loss: 0.1938 - accuracy: 0.9275 - val_loss: 1.2252 - val_accuracy: 0.6487 - 1204s/epoch - 756ms/step\n",
      "Epoch 96/200\n",
      "1592/1592 - 1197s - loss: 0.1977 - accuracy: 0.9263 - val_loss: 1.2097 - val_accuracy: 0.6522 - 1197s/epoch - 752ms/step\n",
      "Epoch 97/200\n",
      "1592/1592 - 1187s - loss: 0.1941 - accuracy: 0.9274 - val_loss: 1.2485 - val_accuracy: 0.6411 - 1187s/epoch - 746ms/step\n",
      "Epoch 98/200\n",
      "1592/1592 - 1194s - loss: 0.1917 - accuracy: 0.9277 - val_loss: 1.2413 - val_accuracy: 0.6494 - 1194s/epoch - 750ms/step\n",
      "Epoch 99/200\n",
      "1592/1592 - 1187s - loss: 0.1944 - accuracy: 0.9275 - val_loss: 1.2031 - val_accuracy: 0.6498 - 1187s/epoch - 746ms/step\n",
      "Epoch 100/200\n",
      "1592/1592 - 1187s - loss: 0.1907 - accuracy: 0.9288 - val_loss: 1.2689 - val_accuracy: 0.6444 - 1187s/epoch - 746ms/step\n",
      "Epoch 101/200\n",
      "1592/1592 - 1192s - loss: 0.1951 - accuracy: 0.9271 - val_loss: 1.2669 - val_accuracy: 0.6469 - 1192s/epoch - 749ms/step\n",
      "Epoch 102/200\n",
      "1592/1592 - 1192s - loss: 0.1941 - accuracy: 0.9276 - val_loss: 1.2274 - val_accuracy: 0.6503 - 1192s/epoch - 749ms/step\n",
      "Epoch 103/200\n",
      "1592/1592 - 1196s - loss: 0.1900 - accuracy: 0.9291 - val_loss: 1.2561 - val_accuracy: 0.6473 - 1196s/epoch - 751ms/step\n",
      "Epoch 104/200\n",
      "1592/1592 - 1190s - loss: 0.1896 - accuracy: 0.9294 - val_loss: 1.2447 - val_accuracy: 0.6480 - 1190s/epoch - 748ms/step\n",
      "Epoch 105/200\n",
      "1592/1592 - 1195s - loss: 0.1928 - accuracy: 0.9279 - val_loss: 1.2375 - val_accuracy: 0.6474 - 1195s/epoch - 751ms/step\n",
      "Epoch 106/200\n",
      "1592/1592 - 1193s - loss: 0.1904 - accuracy: 0.9285 - val_loss: 1.2538 - val_accuracy: 0.6508 - 1193s/epoch - 749ms/step\n",
      "Epoch 107/200\n",
      "1592/1592 - 1192s - loss: 0.2019 - accuracy: 0.9245 - val_loss: 1.1988 - val_accuracy: 0.6516 - 1192s/epoch - 749ms/step\n",
      "Epoch 108/200\n",
      "1592/1592 - 1194s - loss: 0.1848 - accuracy: 0.9305 - val_loss: 1.2731 - val_accuracy: 0.6432 - 1194s/epoch - 750ms/step\n",
      "Epoch 109/200\n",
      "1592/1592 - 1183s - loss: 0.1871 - accuracy: 0.9300 - val_loss: 1.2973 - val_accuracy: 0.6458 - 1183s/epoch - 743ms/step\n",
      "Epoch 110/200\n",
      "1592/1592 - 1193s - loss: 0.1974 - accuracy: 0.9265 - val_loss: 1.2534 - val_accuracy: 0.6508 - 1193s/epoch - 750ms/step\n",
      "Epoch 111/200\n",
      "1592/1592 - 1185s - loss: 0.1877 - accuracy: 0.9295 - val_loss: 1.1957 - val_accuracy: 0.6447 - 1185s/epoch - 744ms/step\n",
      "Epoch 112/200\n",
      "1592/1592 - 1196s - loss: 0.1962 - accuracy: 0.9265 - val_loss: 1.2095 - val_accuracy: 0.6450 - 1196s/epoch - 751ms/step\n",
      "Epoch 113/200\n",
      "1592/1592 - 1192s - loss: 0.1869 - accuracy: 0.9304 - val_loss: 1.2440 - val_accuracy: 0.6478 - 1192s/epoch - 749ms/step\n",
      "Epoch 114/200\n",
      "1592/1592 - 1190s - loss: 0.2002 - accuracy: 0.9253 - val_loss: 1.1975 - val_accuracy: 0.6503 - 1190s/epoch - 747ms/step\n",
      "Epoch 115/200\n",
      "1592/1592 - 1184s - loss: 0.1864 - accuracy: 0.9303 - val_loss: 1.2479 - val_accuracy: 0.6545 - 1184s/epoch - 744ms/step\n",
      "Epoch 116/200\n",
      "1592/1592 - 1188s - loss: 0.1871 - accuracy: 0.9297 - val_loss: 1.2526 - val_accuracy: 0.6462 - 1188s/epoch - 746ms/step\n",
      "Epoch 117/200\n",
      "1592/1592 - 1191s - loss: 0.1844 - accuracy: 0.9308 - val_loss: 1.2947 - val_accuracy: 0.6396 - 1191s/epoch - 748ms/step\n",
      "Epoch 118/200\n",
      "1592/1592 - 1189s - loss: 0.1844 - accuracy: 0.9309 - val_loss: 1.2579 - val_accuracy: 0.6490 - 1189s/epoch - 747ms/step\n",
      "Epoch 119/200\n",
      "1592/1592 - 1185s - loss: 0.1850 - accuracy: 0.9305 - val_loss: 1.3260 - val_accuracy: 0.6459 - 1185s/epoch - 744ms/step\n",
      "Epoch 120/200\n",
      "1592/1592 - 1181s - loss: 0.1883 - accuracy: 0.9296 - val_loss: 1.2514 - val_accuracy: 0.6509 - 1181s/epoch - 742ms/step\n",
      "Epoch 121/200\n",
      "1592/1592 - 1179s - loss: 0.1830 - accuracy: 0.9307 - val_loss: 1.3082 - val_accuracy: 0.6480 - 1179s/epoch - 740ms/step\n",
      "Epoch 122/200\n",
      "1592/1592 - 1186s - loss: 0.1823 - accuracy: 0.9318 - val_loss: 1.3100 - val_accuracy: 0.6388 - 1186s/epoch - 745ms/step\n",
      "Epoch 123/200\n",
      "1592/1592 - 1192s - loss: 0.1816 - accuracy: 0.9320 - val_loss: 1.2835 - val_accuracy: 0.6472 - 1192s/epoch - 749ms/step\n",
      "Epoch 124/200\n",
      "1592/1592 - 1196s - loss: 0.1795 - accuracy: 0.9323 - val_loss: 1.2924 - val_accuracy: 0.6510 - 1196s/epoch - 752ms/step\n",
      "Epoch 125/200\n",
      "1592/1592 - 1190s - loss: 0.1825 - accuracy: 0.9313 - val_loss: 1.2736 - val_accuracy: 0.6516 - 1190s/epoch - 748ms/step\n",
      "Epoch 126/200\n",
      "1592/1592 - 1179s - loss: 0.1827 - accuracy: 0.9320 - val_loss: 1.2766 - val_accuracy: 0.6493 - 1179s/epoch - 741ms/step\n",
      "Epoch 127/200\n",
      "1592/1592 - 1189s - loss: 0.5428 - accuracy: 0.7301 - val_loss: 1.0977 - val_accuracy: 0.3752 - 1189s/epoch - 747ms/step\n",
      "Epoch 128/200\n",
      "1592/1592 - 1192s - loss: 0.9652 - accuracy: 0.4748 - val_loss: 0.9581 - val_accuracy: 0.4997 - 1192s/epoch - 749ms/step\n",
      "Epoch 129/200\n",
      "1592/1592 - 1191s - loss: 0.8703 - accuracy: 0.5473 - val_loss: 0.9156 - val_accuracy: 0.5211 - 1191s/epoch - 748ms/step\n",
      "Epoch 130/200\n",
      "1592/1592 - 1197s - loss: 0.8254 - accuracy: 0.5834 - val_loss: 0.8959 - val_accuracy: 0.5439 - 1197s/epoch - 752ms/step\n",
      "Epoch 131/200\n",
      "1592/1592 - 1193s - loss: 0.6531 - accuracy: 0.7153 - val_loss: 0.8372 - val_accuracy: 0.6644 - 1193s/epoch - 749ms/step\n",
      "Epoch 132/200\n",
      "1592/1592 - 1198s - loss: 0.3387 - accuracy: 0.8716 - val_loss: 1.0173 - val_accuracy: 0.6563 - 1198s/epoch - 753ms/step\n",
      "Epoch 133/200\n",
      "1592/1592 - 1179s - loss: 0.2554 - accuracy: 0.9043 - val_loss: 1.0948 - val_accuracy: 0.6510 - 1179s/epoch - 741ms/step\n",
      "Epoch 134/200\n",
      "1592/1592 - 1176s - loss: 0.2309 - accuracy: 0.9140 - val_loss: 1.1554 - val_accuracy: 0.6553 - 1176s/epoch - 739ms/step\n",
      "Epoch 135/200\n",
      "1592/1592 - 1169s - loss: 0.2140 - accuracy: 0.9201 - val_loss: 1.1773 - val_accuracy: 0.6528 - 1169s/epoch - 734ms/step\n",
      "Epoch 136/200\n",
      "1592/1592 - 1173s - loss: 0.2137 - accuracy: 0.9196 - val_loss: 1.2131 - val_accuracy: 0.6487 - 1173s/epoch - 737ms/step\n",
      "Epoch 137/200\n",
      "1592/1592 - 1165s - loss: 0.2015 - accuracy: 0.9250 - val_loss: 1.2525 - val_accuracy: 0.6426 - 1165s/epoch - 732ms/step\n",
      "Epoch 138/200\n",
      "1592/1592 - 1186s - loss: 0.1980 - accuracy: 0.9255 - val_loss: 1.2679 - val_accuracy: 0.6416 - 1186s/epoch - 745ms/step\n",
      "Epoch 139/200\n",
      "1592/1592 - 1190s - loss: 0.1975 - accuracy: 0.9261 - val_loss: 1.2557 - val_accuracy: 0.6448 - 1190s/epoch - 747ms/step\n",
      "Epoch 140/200\n",
      "1592/1592 - 1191s - loss: 0.2183 - accuracy: 0.9182 - val_loss: 1.1690 - val_accuracy: 0.6532 - 1191s/epoch - 748ms/step\n",
      "Epoch 141/200\n",
      "1592/1592 - 1173s - loss: 0.1913 - accuracy: 0.9285 - val_loss: 1.2409 - val_accuracy: 0.6527 - 1173s/epoch - 737ms/step\n",
      "Epoch 142/200\n",
      "1592/1592 - 1176s - loss: 0.1884 - accuracy: 0.9292 - val_loss: 1.2107 - val_accuracy: 0.6510 - 1176s/epoch - 739ms/step\n",
      "Epoch 143/200\n",
      "1592/1592 - 1180s - loss: 0.1870 - accuracy: 0.9297 - val_loss: 1.2411 - val_accuracy: 0.6458 - 1180s/epoch - 741ms/step\n",
      "Epoch 144/200\n",
      "1592/1592 - 1201s - loss: 0.1865 - accuracy: 0.9300 - val_loss: 1.2786 - val_accuracy: 0.6476 - 1201s/epoch - 754ms/step\n",
      "Epoch 145/200\n",
      "1592/1592 - 1252s - loss: 0.1881 - accuracy: 0.9291 - val_loss: 1.2798 - val_accuracy: 0.6438 - 1252s/epoch - 787ms/step\n",
      "Epoch 146/200\n",
      "1592/1592 - 1232s - loss: 0.1851 - accuracy: 0.9303 - val_loss: 1.2623 - val_accuracy: 0.6482 - 1232s/epoch - 774ms/step\n",
      "Epoch 147/200\n",
      "1592/1592 - 1242s - loss: 0.1843 - accuracy: 0.9304 - val_loss: 1.3168 - val_accuracy: 0.6427 - 1242s/epoch - 780ms/step\n",
      "Epoch 148/200\n",
      "1592/1592 - 1247s - loss: 0.1831 - accuracy: 0.9312 - val_loss: 1.3057 - val_accuracy: 0.6465 - 1247s/epoch - 784ms/step\n",
      "Epoch 149/200\n",
      "1592/1592 - 1246s - loss: 0.2051 - accuracy: 0.9236 - val_loss: 1.1410 - val_accuracy: 0.5924 - 1246s/epoch - 783ms/step\n",
      "Epoch 150/200\n",
      "1592/1592 - 1262s - loss: 0.1987 - accuracy: 0.9257 - val_loss: 1.2655 - val_accuracy: 0.6555 - 1262s/epoch - 793ms/step\n",
      "Epoch 151/200\n",
      "1592/1592 - 1251s - loss: 0.1788 - accuracy: 0.9325 - val_loss: 1.2727 - val_accuracy: 0.6477 - 1251s/epoch - 786ms/step\n",
      "Epoch 152/200\n",
      "1592/1592 - 1253s - loss: 0.1806 - accuracy: 0.9320 - val_loss: 1.2589 - val_accuracy: 0.6500 - 1253s/epoch - 787ms/step\n",
      "Epoch 153/200\n",
      "1592/1592 - 1254s - loss: 0.1771 - accuracy: 0.9336 - val_loss: 1.3375 - val_accuracy: 0.6445 - 1254s/epoch - 787ms/step\n",
      "Epoch 154/200\n",
      "1592/1592 - 1265s - loss: 0.1793 - accuracy: 0.9324 - val_loss: 1.3197 - val_accuracy: 0.6428 - 1265s/epoch - 795ms/step\n",
      "Epoch 155/200\n",
      "1592/1592 - 1234s - loss: 0.1816 - accuracy: 0.9315 - val_loss: 1.2633 - val_accuracy: 0.6492 - 1234s/epoch - 775ms/step\n",
      "Epoch 156/200\n",
      "1592/1592 - 1244s - loss: 0.1770 - accuracy: 0.9340 - val_loss: 1.3460 - val_accuracy: 0.6488 - 1244s/epoch - 781ms/step\n",
      "Epoch 157/200\n",
      "1592/1592 - 1244s - loss: 0.1781 - accuracy: 0.9330 - val_loss: 1.3172 - val_accuracy: 0.6436 - 1244s/epoch - 782ms/step\n",
      "Epoch 158/200\n",
      "1592/1592 - 1240s - loss: 0.1789 - accuracy: 0.9326 - val_loss: 1.3317 - val_accuracy: 0.6500 - 1240s/epoch - 779ms/step\n",
      "Epoch 159/200\n",
      "1592/1592 - 1247s - loss: 0.1793 - accuracy: 0.9330 - val_loss: 1.2932 - val_accuracy: 0.6494 - 1247s/epoch - 783ms/step\n",
      "Epoch 160/200\n",
      "1592/1592 - 1235s - loss: 0.1771 - accuracy: 0.9336 - val_loss: 1.3428 - val_accuracy: 0.6416 - 1235s/epoch - 776ms/step\n",
      "Epoch 161/200\n",
      "1592/1592 - 1238s - loss: 0.1762 - accuracy: 0.9340 - val_loss: 1.3248 - val_accuracy: 0.6463 - 1238s/epoch - 778ms/step\n",
      "Epoch 162/200\n",
      "1592/1592 - 1246s - loss: 0.1759 - accuracy: 0.9340 - val_loss: 1.2949 - val_accuracy: 0.6489 - 1246s/epoch - 783ms/step\n",
      "Epoch 163/200\n",
      "1592/1592 - 1241s - loss: 0.1732 - accuracy: 0.9352 - val_loss: 1.3165 - val_accuracy: 0.6512 - 1241s/epoch - 780ms/step\n",
      "Epoch 164/200\n",
      "1592/1592 - 1251s - loss: 0.1737 - accuracy: 0.9348 - val_loss: 1.3251 - val_accuracy: 0.6496 - 1251s/epoch - 786ms/step\n",
      "Epoch 165/200\n",
      "1592/1592 - 1248s - loss: 0.1736 - accuracy: 0.9348 - val_loss: 1.3316 - val_accuracy: 0.6425 - 1248s/epoch - 784ms/step\n",
      "Epoch 166/200\n",
      "1592/1592 - 1241s - loss: 0.1732 - accuracy: 0.9350 - val_loss: 1.3607 - val_accuracy: 0.6483 - 1241s/epoch - 780ms/step\n",
      "Epoch 167/200\n",
      "1592/1592 - 1244s - loss: 0.1749 - accuracy: 0.9343 - val_loss: 1.3197 - val_accuracy: 0.6503 - 1244s/epoch - 781ms/step\n",
      "Epoch 168/200\n",
      "1592/1592 - 1249s - loss: 0.1712 - accuracy: 0.9355 - val_loss: 1.3188 - val_accuracy: 0.6517 - 1249s/epoch - 785ms/step\n",
      "Epoch 169/200\n",
      "1592/1592 - 1245s - loss: 0.1855 - accuracy: 0.9304 - val_loss: 1.3184 - val_accuracy: 0.6452 - 1245s/epoch - 782ms/step\n",
      "Epoch 170/200\n",
      "1592/1592 - 1243s - loss: 0.1688 - accuracy: 0.9368 - val_loss: 1.3239 - val_accuracy: 0.6465 - 1243s/epoch - 781ms/step\n",
      "Epoch 171/200\n",
      "1592/1592 - 1243s - loss: 0.1716 - accuracy: 0.9348 - val_loss: 1.3288 - val_accuracy: 0.6424 - 1243s/epoch - 781ms/step\n",
      "Epoch 172/200\n",
      "1592/1592 - 1257s - loss: 0.1720 - accuracy: 0.9350 - val_loss: 1.3305 - val_accuracy: 0.6491 - 1257s/epoch - 789ms/step\n",
      "Epoch 173/200\n",
      "1592/1592 - 1297s - loss: 0.1716 - accuracy: 0.9351 - val_loss: 1.3509 - val_accuracy: 0.6539 - 1297s/epoch - 815ms/step\n",
      "Epoch 174/200\n",
      "1592/1592 - 1300s - loss: 0.1698 - accuracy: 0.9362 - val_loss: 1.3513 - val_accuracy: 0.6452 - 1300s/epoch - 816ms/step\n",
      "Epoch 175/200\n",
      "1592/1592 - 1299s - loss: 0.1693 - accuracy: 0.9359 - val_loss: 1.3474 - val_accuracy: 0.6481 - 1299s/epoch - 816ms/step\n",
      "Epoch 176/200\n",
      "1592/1592 - 1286s - loss: 0.1719 - accuracy: 0.9358 - val_loss: 1.3349 - val_accuracy: 0.6498 - 1286s/epoch - 808ms/step\n",
      "Epoch 177/200\n",
      "1592/1592 - 1301s - loss: 0.1711 - accuracy: 0.9356 - val_loss: 1.2987 - val_accuracy: 0.6513 - 1301s/epoch - 817ms/step\n",
      "Epoch 178/200\n",
      "1592/1592 - 1292s - loss: 0.1673 - accuracy: 0.9366 - val_loss: 1.3280 - val_accuracy: 0.6533 - 1292s/epoch - 812ms/step\n",
      "Epoch 179/200\n",
      "1592/1592 - 1265s - loss: 0.1687 - accuracy: 0.9369 - val_loss: 1.3685 - val_accuracy: 0.6424 - 1265s/epoch - 794ms/step\n",
      "Epoch 180/200\n",
      "1592/1592 - 1280s - loss: 0.1681 - accuracy: 0.9365 - val_loss: 1.3160 - val_accuracy: 0.6501 - 1280s/epoch - 804ms/step\n",
      "Epoch 181/200\n",
      "1592/1592 - 1276s - loss: 0.1684 - accuracy: 0.9369 - val_loss: 1.3485 - val_accuracy: 0.6463 - 1276s/epoch - 802ms/step\n",
      "Epoch 182/200\n",
      "1592/1592 - 1288s - loss: 0.1695 - accuracy: 0.9364 - val_loss: 1.3729 - val_accuracy: 0.6430 - 1288s/epoch - 809ms/step\n",
      "Epoch 183/200\n",
      "1592/1592 - 1288s - loss: 0.1699 - accuracy: 0.9366 - val_loss: 1.3596 - val_accuracy: 0.6426 - 1288s/epoch - 809ms/step\n",
      "Epoch 184/200\n",
      "1592/1592 - 1297s - loss: 0.1661 - accuracy: 0.9379 - val_loss: 1.3443 - val_accuracy: 0.6450 - 1297s/epoch - 814ms/step\n",
      "Epoch 185/200\n",
      "1592/1592 - 1298s - loss: 0.1657 - accuracy: 0.9380 - val_loss: 1.3950 - val_accuracy: 0.6445 - 1298s/epoch - 815ms/step\n",
      "Epoch 186/200\n",
      "1592/1592 - 1288s - loss: 0.1675 - accuracy: 0.9363 - val_loss: 1.3486 - val_accuracy: 0.6460 - 1288s/epoch - 809ms/step\n",
      "Epoch 187/200\n",
      "1592/1592 - 1299s - loss: 0.1675 - accuracy: 0.9373 - val_loss: 1.3372 - val_accuracy: 0.6450 - 1299s/epoch - 816ms/step\n",
      "Epoch 188/200\n",
      "1592/1592 - 1290s - loss: 0.1685 - accuracy: 0.9368 - val_loss: 1.3420 - val_accuracy: 0.6462 - 1290s/epoch - 810ms/step\n",
      "Epoch 189/200\n",
      "1592/1592 - 1283s - loss: 0.1637 - accuracy: 0.9380 - val_loss: 1.3832 - val_accuracy: 0.6460 - 1283s/epoch - 806ms/step\n",
      "Epoch 190/200\n",
      "1592/1592 - 1280s - loss: 0.1701 - accuracy: 0.9361 - val_loss: 1.3139 - val_accuracy: 0.6467 - 1280s/epoch - 804ms/step\n",
      "Epoch 191/200\n",
      "1592/1592 - 1280s - loss: 0.1632 - accuracy: 0.9385 - val_loss: 1.3558 - val_accuracy: 0.6453 - 1280s/epoch - 804ms/step\n",
      "Epoch 192/200\n",
      "1592/1592 - 1287s - loss: 0.1690 - accuracy: 0.9359 - val_loss: 1.3336 - val_accuracy: 0.6501 - 1287s/epoch - 809ms/step\n",
      "Epoch 193/200\n",
      "1592/1592 - 1301s - loss: 0.1624 - accuracy: 0.9382 - val_loss: 1.4119 - val_accuracy: 0.6466 - 1301s/epoch - 817ms/step\n",
      "Epoch 194/200\n",
      "1592/1592 - 1307s - loss: 0.1688 - accuracy: 0.9364 - val_loss: 1.3467 - val_accuracy: 0.6442 - 1307s/epoch - 821ms/step\n",
      "Epoch 195/200\n",
      "1592/1592 - 1305s - loss: 0.1625 - accuracy: 0.9390 - val_loss: 1.3908 - val_accuracy: 0.6379 - 1305s/epoch - 819ms/step\n",
      "Epoch 196/200\n",
      "1592/1592 - 1302s - loss: 0.1720 - accuracy: 0.9355 - val_loss: 1.3326 - val_accuracy: 0.6479 - 1302s/epoch - 818ms/step\n",
      "Epoch 197/200\n",
      "1592/1592 - 1288s - loss: 0.1656 - accuracy: 0.9374 - val_loss: 1.3523 - val_accuracy: 0.6527 - 1288s/epoch - 809ms/step\n",
      "Epoch 198/200\n",
      "1592/1592 - 1290s - loss: 0.1741 - accuracy: 0.9345 - val_loss: 1.3117 - val_accuracy: 0.6505 - 1290s/epoch - 810ms/step\n",
      "Epoch 199/200\n",
      "1592/1592 - 1296s - loss: 0.1602 - accuracy: 0.9397 - val_loss: 1.3828 - val_accuracy: 0.6480 - 1296s/epoch - 814ms/step\n",
      "Epoch 200/200\n",
      "1592/1592 - 1303s - loss: 0.1742 - accuracy: 0.9341 - val_loss: 1.3298 - val_accuracy: 0.6502 - 1303s/epoch - 819ms/step\n",
      "CPU times: total: 38d 10h 10min 6s\n",
      "Wall time: 2d 19h 37min 32s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1a4989b3f50>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='auto',\n",
    "    save_best_only=True)\n",
    "\n",
    "biadeeplob.fit(trainX_CNN, trainY_CNN, validation_data=(valX_CNN, valY_CNN),\n",
    "            epochs=200, batch_size=128, verbose=2, callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w-26dzYHLek4",
    "outputId": "d6d7644d-825a-46e3-f6cc-2d111a44ae65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4359/4359 [==============================] - 275s 57ms/step\n"
     ]
    }
   ],
   "source": [
    "biadeeplob.load_weights(checkpoint_filepath)\n",
    "pred = biadeeplob.predict(testX_CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XX2UQWkJLizk",
    "outputId": "2f76e22c-28cf-4bd2-8140-ae6943d00e36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: 0.7600223675154852\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7717    0.7212    0.7456     47915\n",
      "           1     0.8353    0.7541    0.7926     48050\n",
      "           2     0.6862    0.8093    0.7427     43523\n",
      "\n",
      "    accuracy                         0.7600    139488\n",
      "   macro avg     0.7644    0.7615    0.7603    139488\n",
      "weighted avg     0.7669    0.7600    0.7609    139488\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('accuracy_score:', accuracy_score(np.argmax(testY_CNN, axis=1), np.argmax(pred, axis=1)))\n",
    "print(classification_report(np.argmax(testY_CNN, axis=1), np.argmax(pred, axis=1), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "qkha5y5ILSi6",
    "outputId": "5413b0e9-b71b-428c-9fe1-dd2e624e01bf"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "c = confusion_matrix(np.argmax(testY_CNN, axis=1), np.argmax(pred, axis=1), normalize=\"true\")\n",
    "disp = ConfusionMatrixDisplay(c)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
